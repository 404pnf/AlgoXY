\ifx\wholebook\relax \else
% ------------------------ 

\documentclass{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
%\input{../common.tex}
\input{../common-en.tex}

\setcounter{page}{1}

\begin{document}

\fi
%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{Searching}

\author{Larry~LIU~Xinyu
\thanks{{\bfseries Larry LIU Xinyu } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\markboth{Searching}{AlgoXY}

\maketitle

\ifx\wholebook\relax
\chapter{Searching}
\numberwithin{Exercise}{chapter}
\fi

% ================================================================
%                 Introduction
% ================================================================
\section{Introduction}
\label{introduction} 
Searching is a quite big and important area. Computer turns many hard
searching problem realistic, which is almost impossible for human begins.
A modern industry robot can even search and pick the correct gadget from
the pipeline for assembly; A GPS car navigator can search among the
map, for the best route to a specific place. The modern mobile phone
does not only equipped with such map navigator, but can also search
for internet shopping with the best price.

This chapter just scratch the surface of elementary searching. One
good thing that computer offers is the brute-force scanning for a
certain result in a large sequences. The divide and conquers
search strategy will be briefed with two problems, one is to find
the $k$-th big one among a list of unsorted elements; the other
is the popular binary search among a list of sorted elements.
We'll also introduce the extension of binary search for searching
among multiple-dimension data.

Text matching is also very important in our daily life, two well-known
searching algorithms, Knuth-Morris-Pratt (KMP) and Boyer-Moore algorithm
will be introduced, They set good example for another searching strategy:
information reusing.

Besides sequence search, some elementary methods for searching
solution of some interesting problems will be introduced. They
were mostly well studied in the early phase of AI (artificial
intelligence), including the basic DFS (Depth first search), 
and BFS (Breadth first search).

Finally, Dynamic programming will be briefed for searching
optimal solutions, and we'll also introduce about greedy
algorithm which is applicable for some special cases.

All algorithms will be realized in both imperative and functional
approaches.

% ================================================================
% Sequence search
% ================================================================
\section{Sequence search}
Although modern computer offers fast speed for brute-force searching,
and even if the Moore's law could be strictly followed, the grows of
huge data is too fast to be handled well in this way. We've seen
a vivid example in the introduction chapter of this book.
It's why people study the computer search algorithms. 

\subsection{Divide and conquer search}
One solution is to use divide and conquer approach. That if we can
repeatedly scale down the search domain, the data being dropped needn't
be examined at all. This will definately speed up the search.

\subsubsection{$k$-selection problem}
\index{Selection algorithm}
Consider a problem of finding the $k$-th biggest element among $n$ data.
The most straightforward idea is to find the minimum one first, then
drop it and find the second minimum element among the rest. Repeat
this minimum finding and dropping $k$ steps will find the
$k$-th smallest one. Finding the minimum element among $n$ data
costs linear $O(n)$ time. Thus this method performs $O(kn)$ time,
if $k$ is much smaller than $n$.

Another method is to use the `heap' data structure we've introduced.
No matter what concreate heap is used, binary heap with implicit array,
Fibonacci heap or others, a top element accessing followed by a 
popping is typically bound $O(lg n)$ time. Thus this method, as
formalized in euqation (\ref{eq:kth-heap1}) and (\ref{eq:kth-heap2}) performs in $O(k \lg n)$ time, if
$k$ is much smaller than $n$.
    
\be
top(k, L) = find(k, heapify(L))
\label{eq:kth-heap1}
\ee

\be
find(k, H) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  top(H) & k = 0 \\
  find(k-1, pop(H)) & otherwise
  \end{array}
\right.
\label{eq:kth-heap2}
\ee

However, heap adds some complexity to the solution. Is there
any simple, fast method to find the $k$-th element?

The divide and conquer strategy can help us. If we can divide all the
elements into two sub lists $A$ and $B$, and ensure all the elements in 
$A$ is not greater than any elements in $B$, we can scale down the
problem by following this method\footnote{This actually demands a more accurate
definition of the $k$-th biggest in $L$: It's equal to the $k$-the element
of $L'$, where $L'$ is a permutation of $L$, and $L'$ is in monotonic non-decreasing order.}:

\begin{enumerate}
\item Compare the length of sub list $A$ and $k$;
\item If $k < |A|$, the $k$-th biggest one must contained in $A$, we can drop $B$ and {\em further search} in $A$;
\item If $|A| < k$, the $k$-th biggest one must contained in $B$, we can drop $A$ and {\em further serach} the $(k-|A|)$-th
biggest one in $B$.
\end{enumerate}

Note that the {\em italic font} emphasizes the fact of recursion. The ideal case always divide
the list into two equaly big sub lists $A$ and $B$, so that we can halve the problem each time.
Such ideal case leads to a performance of $O(n)$ linear time. 

Thus the key problem is how to realize dividing, which collect the first $m$ biggest elements in one sub list,
and put the rest in another.

This reminds us the partition algorithm in quick sort, which moves all the elements smaller than the
pivot infront of it, and moves those greater than the pivot after it. Based on this idea, we can
refine a divide and conquer $k$-selection algorithm, which is called quick selection algorithm.

\begin{enumerate}
\item Randomly select an element (the first for instance) as the pivot;
\item Moves all elements which aren't greater than the pivot in a sub list $A$; and moves the rest to sub list $B$;
\item Compare the length of $A$ with $k$, if $|A| = k - 1$, then the pivot is the $k$-th biggest one;
\item If $|A| > k - 1$, recusively find the $k$-th biggest one among $A$;
\item Otherwise, recursively find the $(k - |A|)$-th biggest one among $B$;
\end{enumerate}

This algorithm can be formalized in below equation. Suppose $0 < k \leq |L|$ , where $L$ is a non-empty list of elements.
Denote $l_1$ as the first element in $L$. It is chosen as the pivot; $L'$ contains the rest elements except for $l_1$.
$(A, B) = partition(\lambda_x \cdot x \leq l_1, L')$ partitions $L'$ by using the same algorithm defined
in the chapter of quick sort.

\be
top(k, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  l_1 & |A| = k - 1 \\
  top(k - 1 - |A|, B) & |A| < k - 1 \\
  top(k, A) & otherwise
  \end{array}
\right.  
\ee

\be
partition(p, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, \Phi) & L = \Phi \\
  (\{ l_1 \} \cup A, B) & p(l_1), (A, B) = partition(p, L') \\
  (A, \{ l_1 \} \cup B) & \lnot p(l_1)
  \end{array}
\right.  
\ee

The following Haskell example program implements this algorithm.

\lstset{language=Haskell}
\begin{lstlisting}
top n (x:xs) | len == n - 1 = x
             | len < n - 1 = top (n - len - 1) bs
             | otherwise = top n as
    where                           
      (as, bs) = partition (<=x) xs
      len = length as
\end{lstlisting}

The partition function is provided in Haskell standard library, the detailed implementation
can be referred to previous chapter about quick sort.

The lucky case is that, the $k$-th biggest element is selected as the pivot at be very beginning.
The partition function examines the whole list, and finds that there are $k-1$ elements not greater
than the pivot, we are done after $O(n)$ time. The worst case is that either the maximum or the
minimum elements are selected as the pivot every time. The partition always produces an empty
sub list, that either $A$ or $B$ is empty. If we always pick the minimum as the pivot, the
performance is bound to $O(kn)$. If we always pick the maximum as the pivot, the performance
is $O((n-k)n)$. If $k$ is much less than $n$, it downgrads to quadratic $O(n^2)$ time.

The best case (not the lucky case), is that the pivot always partition the list perfectly.
The length of $A$ is nearly same as the length of $B$. In such case, the list is halved
every time. It needs about $O(\lg n)$ partitions, each partition takes linear time proportion
to the length of the halved list. This can be expressed as 
$O(n + \frac{n}{2} + \frac{n}{4} + ... + \frac{n}{2^m})$, where $m$ is the biggest number satisfies
$\frac{n}{2^m} < k$. Summing the series leads to the result of $O(n)$.

The average case analysis needs tool of mathematical expectation. It's quite similiar to the
proof given in previous chapter of quick sort. It's left as an exercise to the reader.

Similar as quick sort, this divide and conquer selection algorithm performs well most time
in practice. We can take the same engineering practice such as media-of-three, or randomly
pivoting as we did for quick sort. Below is the imperative realization for example.

\begin{algorithmic}[1]
\Function{Top}{$k, A, l, u$}
  \State \textproc{Exchange} $A[l] \leftrightarrow A[$ \Call{Random}{$l, u$} $]$ \Comment{Randomly select in $[l, u]$}
  \State $p \gets$ \Call{Partition}{$A, l, u$}
  \If{$p - l + 1 = k$}
    \State \Return $A[p]$
  \EndIf
  \If{$k < p - l + 1$}
    \State \Return \Call{Top}{$k, A, l, p-1$}
  \EndIf
  \State \Return \Call{Top}{$k - p + l - 1, A, p + 1, u$}
\EndFunction
\end{algorithmic}

This algorithm searches the $k$-th biggest element in range of $[l, u]$ for array $A$. The boundaries
are included. It first randomly selects a position, and swaps it with the first one. Then this element
is chosen as the pivot for partitioning. The partition algorithm in-place moves elements and 
returns the position where the pivot being moved. If the pivot is just located at the $k$-th, then
we are done; if there are more than $k-1$ elements not greater than the pivot, the algorithm 
recursively search the $k$-th biggest one in range $[l, p-1]$; otherwise, $k$ is deduced by the
number of elements before the pivot, and recusively search the range after the pivot $[p+1, u]$.

There are many methods to realize the partition algorithm, below one is based on N. Lumoto's method.
Other realizations are left as exercises to the reader.

\begin{algorithmic}[1]
\Function{Partition}{A, l, u}
  \State $p \gets A[l]$ 
  \State $L \gets l$ 
  \For{$R \in [l+1, u]$} 
    \If{$\lnot (p < A[R])$} 
      \State $L \gets L + 1$
      \State \textproc{Exchange} $A[L] \leftrightarrow A[R]$
    \EndIf
  \EndFor
  \State \textproc{Exchange} $A[L] \leftrightarrow p$
  \State \Return $L$
\EndFunction
\end{algorithmic}

Below ANSI example program implements this algorithm. Note that it handles the special case that
either the array is empty, or $k$ is out of the boundaries of the array. It returns -1 to indicate
the search failure.

\lstset{language=C}
\begin{lstlisting}
int partition(Key* xs, int l, int u) {
    int r, p = l;
    for (r = l + 1; r < u; ++r)
        if (!(xs[p] < xs[r]))
            swap(xs, ++l, r);
    swap(xs, p, l);
    return l;
}

/* The result is stored in xs[k], returns k if u-l >=k, otherwise -1 */
int top(int k, Key* xs, int l, int u) {
    int p;
    if (l < u) {
        swap(xs, l, rand() % (u - l) + l);
        p = partition(xs, l, u);
        if (p - l + 1 == k) 
            return p;
        return (k < p - l + 1) ? top(k, xs, l, p) : 
                                 top(k- p + l - 1, xs, p + 1, u);
    }
    return -1;
}
\end{lstlisting}

TODO: Variants: get the top k elements

TODO: Partition by min/max, to exercise???

\begin{verbatim}
function nth(xs, k)
  l <- 1
  u <- length xs
  loop
    x <- max(xs[1..k])
    y <- min(xs[k+1..u])
    if x < y then
      return
    else
      swap(x, y)
      l <- partition xs[1..k] with x
      u <- partition xs[k_1..n] with y 
\end{verbatim}

\subsubsection{binary search}

\subsubsection{2 demension bineary search? Dijkstra}

\begin{Exercise}
\begin{itemize}
\item Prove that the average case of the divide and conquer solution to $k$-selection problem is $O(n)$. Please refer to previous chapter about quick sort.
\item Implement the imperative $k$-selection problem with 2-way partition, and median-of-three pivot selection.
\item Implement the imperative $k$-selection problem to handle duplicated elements effectively.
\end{itemize}
\end{Exercise}

\subsection{Information reuse}

\subsubsection{KMP}

\subsubsection{Boyer-moore}

\section{Solution searching}
\subsection{DFS and BFS}

\subsection{Search the optimal solution}

\subsection{Dynamic programming}

\subsection{Grady algorithm}

\section{Short summary} 
summary

\begin{thebibliography}{99}

\bibitem{TAOCP}
Donald E. Knuth. ``The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Edition)''. Addison-Wesley Professional; 2 edition (May 4, 1998) ISBN-10: 0201896850 ISBN-13: 978-0201896855

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein. 
``Introduction to Algorithms, Second Edition''. ISBN:0262032937. The MIT Press. 2001

\end{thebibliography}

\ifx\wholebook\relax\else
\end{document}
\fi

