Hi,

现在主流的函数库要么使用merge sort，要么使用quick sort。使用插入排序(insertion sort)的几乎没有。
Insertion sort的性能，按照教科书上的说法是O(N^2)。是比较慢的一种，但是了解Insertion Sort的思路和演化，对我们学习基本算法很有帮助。

Insertion sort可以说是诸多sort算法中的hello world。

有读者问：不是bubble sort是最简单的么？Don Knuth强烈建议在各种教科书中，以及学校的课程中删除Bubble sort的内容。我比较赞同这个观点，Bubble sort误人不浅，它唯一的作用是揭示了如何写一个差劲的程序。可是为什么我们要让初学者上来就了解一个反面教材呢？

Insertion sort的思路非常直观，CLRS上用了一张插图，非常直观的解释了这个思路：假设我们玩扑克时抓牌，每抓到一张新牌，我们就按照大小顺序，插入到手中以抓的牌中，当抓完所有的牌后，我们手中的牌就是按照顺序排好了的。

下面的问题是：普通人在插入新牌的时候，究竟是怎么找到那个合适的位置的？这个是我要讨论的核心之一。

教课书的的算法通常如下：
def isort(xs):
    n = len(xs)
    for i in range(1, n):
        # insert xs[i] to xs[0..i-1]
        x = xs[i]
        j = i - 1
        while j >= 0 and x < xs[j]:
            xs[j+1] = xs[j]
            j = j - 1
        xs[j+1] = x
    return xs

注意：最后那个return可以不写，这是一个in-place的排序算法，这个算法对应的插入牌的顺序是，我们从大到小找出应该插入和合适位置。
那么为什么不从小到大呢？这样不更自然么？

问题是底层的数据结构，我们用数组来表达待排序序列，数组的特点决定了插入操作是高成本的――我们必须shift获取一个空外置后，才能把新元素插入进去。
如果我们从头找待插入的位置i, 然后我们必须把i后面的所有元素向后shift，然后再插入。这样我们就遍历了整个已排序部分的数组；
而如果从后面查找，可以一边查找，一边shift，只要达到i这个位置，就可以结束了，这样就有很大的几率不用遍历整个已排序不分的数组。

这里还有一些小的细节，值得探讨，比较下面的C程序：
void isort(Key* xs, int n){
  int i, j;
  for(i=1; i<n; ++i)
    for(j=i-1; j>=0 && xs[j+1] < xs[j]; --j)
      swap(xs, j, j+1);
}

虽然这个C程序看似比Python那个短小，简洁。但是效率上却略逊色一些。每个swap操作，如果我们假设是使用中间变量temp = x; x=y; y=temp这种形式，
则我们总共进行3*N次赋值操作。其中N是内循环次数

而Python那个，不是每次swap，而是shift，总共进行了N+2次赋值。

当然，我们也可以写一个效率略低，但是更加明确的insert sort，如下：
def isort1(xs):
    ys = []
    for x in xs:
        insert(ys, x)
    return ys

def insert(xs, x):
    xs.append(x)
    i = len(xs) - 1
    while i>0 and xs[i] < xs[i-1]:
        xs[i], xs[i-1] = xs[i-1], xs[i] #swap
        i = i - 1

这本质上和C的那个没有区别。这里不再赘述。

现在回到上面那个核心问题：普通人究竟是怎么找到那个合适的插入位置的。我相信不是依次扫描遍历。下面这个假设可能更加接近：
既然手里的牌是已经排好序的，那么二分查找自然是一个不错的选择：

def isort2(xs):
    n = len(xs)
    for i in range(1, n):
        # insert xs[i] to xs[0..i-1]
        x = xs[i]
        p = binary_search(xs[:i], x)
        for j in range(i, p, -1):
            xs[j] = xs[j-1]
        xs[p] = x
    return xs

# modified binary search, x may not exist in xs
def binary_search(xs, x):
    l = 0
    u = len(xs)
    while l < u:
        m = (l+u)/2
        if xs[m] == x:
            return m # find a duplicated element
        elif xs[m] < x:
            l = m + 1
        else:
            u = m
    return l

这个算法，虽然还是O(N^2)的，但是其实略有提高：
我们原来用了O(N^2)次比较，和O(N^2)次Move，但是由于二分查找的特性，这个小小的改使得我们只用了O(N * lg N)次的比较，和O(N^2)次的Move

有什么办法可以减少Move的次数呢？Move之所以多，是因为我们使用了数组作为底层数据结构，插入操作自然是O(N)的，而如果使用链表，插入操作就能提高到O(1)：

isort [] = []
isort (x:xs) = insert x $ isort xs where
    insert x [] = [x]
    insert x (y:ys) = if x < y then x:y:ys else y:insert x ys

插入的时间节省了，但是比较的时间却还是O(N^2)，这是因为，链表不是random access的数据结构，我们无法对链表进行二分查找。

问题似乎无法解决了，但是我们还有一个终极方案：
   二叉树本质上(naturally )提供了二分查找的功能，并且对二叉树的指定位置进行插入操作是常熟时间O(1)的。
   所以，我们可以把手中抓好的牌放到一个二叉树里，每次抓到新牌，就插入到这个二叉树中。

于是插入排序演化成了树排序，其算法复杂度终于达到了下限――O(N * lg N)！

本outline 完。

另：这样的讲解并非我的独创，而是Knuth先生的思路。来自TAOCP
