\ifx\wholebook\relax \else
% ------------------------ 

\documentclass{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
%\input{../../../common.tex}
\input{../../common-en.tex}

\setcounter{page}{1}

\begin{document}

\fi
%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{Divide and conquer, Quick sort V.S. Merge sort}

\author{Liu~Xinyu
\thanks{{\bfseries Liu Xinyu } \newline
  Email: liuxinyu95@gmail.com \newline}
  }

\markboth{Quick sort V.S. Merge sort}{AlgoXY}

\maketitle

\ifx\wholebook\relax
\chapter{Divide and conquer, Quick sort V.S. Merge sort}
\numberwithin{Exercise}{chapter}
\fi

% ================================================================
%                 Introduction
% ================================================================
\section{Introduction}
\label{introduction} 

It's proved that the best approximate performance of comparison based sorting is $O(N \lg N)$ \cite{TAOCP}.
In this chapter, two divide and conquer sorting algorithms are introduced. Both of them
perform in $O(N \lg N)$ time. One is quick sort. It is
the most popular sorting algorithm. Quick sort has been well studied, many programming libraries provide
sorting tool based on quick sort. 

In this chapter, we'll first introduce the idea of of quick sort, which demonstrates the power of divide 
and conquer strategy well. Several variants will be explained, and we'll see when quick sort performs poor
in some special cases. That the algorithm is not able to partition the sequence in balance.

In order to solve the unbalanced partition problem, we'll next introduce about merge sort, which ensure
the sequence to be well partitioned in all the cases. Some variants of merge sort, including nature merge
sort, bottom-up merge sort are shown as well.

Similar as other chapters, all the algorithm will be realized in both imperative and functional approaches.

% ================================================================
% Quick sort
% ================================================================
\section{Quick sort}
\index{Quick sort}

Consider a teacher arranges a group of kids in kindergarten to stand in a line for some game.
The kids need stand in order of their heights, that the shortest one stands on the left most,
while the tallest stands on the right most. How can the teacher instruct these kids, so that
they can stand in a line by themselves?

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.8]{img/kids-inline.eps}
 \caption{Instruct kids to stand in a line}
 \label{fig:knuth-ssort}
\end{figure}

There are many strategy, and the quick sort approach can be applied here:

\begin{enumerate}
  \item The first kid raises his/her hand. The kids who are shorter than him/her stands to the left to this child; the kids who are taller than him/her stands to the right of this child;
  \item All the kids move to the left if there are repeat the above step; all the kids move to the right repeat the same step as well.
\end{enumerate}

Suppose a group of kids with their heights as $\{102, 100, 98, 95, 96, 99, 101, 97\}$ with [cm] as the unit. 
The following table illustrate how they stand in order of height by folllowing this method.

\begin{tabular}{ | c c c c c c c c |}
\hline
{\bf 102} & 100 & 98 & 95 & 96 & 99 & 101 & 97 \\
{\bf 100} & 98 & 95 & 96 & 99 & 101 & 97 & {\em 102} \\
{\bf 98} & 95 & 96 & 99 & 97 & {\em 100} & 101 & {\em 102} \\
{\bf 95} & 96 & 97 & {\em 98} & 99 & {\em 100} & {\em 101} & {\em 102} \\
{\em 95} & {\bf 96} & 97 & {\em 98} & {\em 99} & {\em 100} & {\em 101} & {\em 102} \\
{\em 95} & {\em 96} & 97 & {\em 98} & {\em 99} & {\em 100} & {\em 101} & {\em 102} \\
{\em 95} & {\em 96} & {\em 97} & {\em 98} & {\em 99} & {\em 100} & {\em 101} & {\em 102} \\
\hline
\end{tabular}

At the beginning, the first child with height 102 cm raises his/her hand. We call this kid the pivot and mark
his height in bold.
It happens that this is the tallest kid.
So all others stands to the left side, which is represented in the second row of above table. Note that
the child with height 102 cm is in the final ordered position, thus we mark it italic. Next the kid with
height 100 cm raise hand, so the children of heights 98, 95, 96 and 99 cm stand to his/her left, and there
is only 1 child of height 101 cm who is taller than this pivot kid. So he stands to the right hand.
The 3rd row in the table shows this stage accordingly. After that, the child of 98 cm high is seleced
as pivot on left hand; while the child of 101 cm high on the right is selected as pivot. Since there
are no other kids in the unsorted group with 101 cm as pivot, this small group is ordered already and
the kid of height 101 cm is in the final proper postion. The same method is applied to the group of kids
which haven't been in correct order until all of them are stands in the final position.

\subsection{Basic version}
Summarize the above insruction leads to the recursive description of quick sort. In order to sort a sequence
of elements $L$.

\begin{itemize}
\item If $L$ is empty, the result is obviously empty; This is the trivial edge case;
\item Otherwise, select an arbitrary element in $L$ as a pivot, recursively sort all elements not greater than $L$, put
the result on the left hand of the pivot, {\em and} recursively sort all elements which are greater than $L$, put
the result on the right hand of the pivot.
\end{itemize}

Note that the emphasized word {\em and}, we don't use `then' here, which indicate it's quite OK that 
the recursive sort on the left and right can be done in parrallel. We'll return this parallism topic soon.

Quick sort was first developed by C. A. R. Hoare in 1960 \cite{TAOCP}, \cite{wiki-qs}. What we describe here
is a basic version. Note that it doesn't state how to select the pivot. We'll see soon that the pivot selection
affects the performance of quick sort later.

The most simple method to select the pivot is always choose the first one so that quick sort can be formalized 
as the following.

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & L = \Phi \\
  sort(\{ x | x \in L', x \leq l_1 \} \cup \{ l_1 \} \cup sort(\{ x | x \in L', l_1 < x \}) & otherwise \\
  \end{array}
\right.  
\ee

Where $l_1$ is the first element of the non-empty list $L$, and $L'$ contains the rest elements $\{l_2, l_3, ...\}$.
Note that we use Zermelo Frankel expression (ZF expresssion for short), which is also known as list
comprehension. A ZF expression $\{ a | a \in S, p_1(a), p_2(a), ... \}$ means take all element in set $S$,
if it satisfies the predication $p_1, p_2, ...$. The result is also a list. Please refer to the appendix
about list in this book for detail.

It's quite straightforward to translate this equation to real code if list comprehension is supported.
The following Haskell code is give for example:

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = sort [y | y<-xs, y <= x] ++ [x] ++ sort [y | y<-xs, x < y]
\end{lstlisting}

This might be the shortest quick sort program in the world at the time when this book is written. Even
a verbose version is still very expressive:

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = as ++ [x] ++ bs where
    as = sort [ a | a <- xs, a <= x]
    bs = sort [ b | b <- xs, x < b]
\end{lstlisting}

There are some variants of this basic quick sort program, such as using explicit filtering instead of
list comprehension. The following Python program demonstrates this for example:

\lstset{language=Python}
\begin{lstlisting}
def sort(xs):
    if xs == []:
        return []
    pivot = xs[0]
    as = sort(filter(lambda x : x <= pivot, xs[1:]))
    bs = sort(filter(lambda x : pivot < x, xs[1:]))
    return as + [pivot] + bs
\end{lstlisting}

\subsection{Strict weak ordering}
We assume the elements are sorted in monotonic none decreasing order so far. It's quite possible to customize
the algorithm, so that it can sort the elements in other ordering criteria. This is necessary in practice because
users may sort numbers, strings, or other complex objects (even list of lists for example) and so on.

The typical generic solution is to abstract the comparison as a parameter as we mentioned in chapters about 
insertion sort and selection sort. Although it needn't the total ordering, the comparison must satisfy 
{\em strict weak ordering} at least \cite{wiki-total-order} \cite{wiki-sweak-order}.

For the sake of brevity, we only considering sort the elements by using less than or equal 
(equivelent to not greater than) in the rest of the chapter.

\subsection{Partition}
\index{Quick sort!partition}
Observing that the basic version actually takes two passes to find all elements not greater than the pivot and 
the rest. Such partition can be accomplished by only one pass. We explicitly define the partition as below.

\be
partition(p, L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\Phi, \Phi) & L = \Phi \\
  (\{ l_1 \} \cup A, B) & p(l_1), (A, B) = partition(p, L') \\
  (A, \{ l_1 \} \cup B) & \lnot p(l_1)
  \end{array}
\right.  
\ee

Note that the operation $\{x\} \cup L$ is just a `cons' operation, which only takes constant time.
The quick sort can be modified accordingly.

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & L = \Phi \\
  sort(A) \cup \{l_1\} \cup sort(B) & otherwise, (A, B) = partition(\lambda_x x \leq l_1, L')
  \end{array}
\right.  
\ee

Translating this new algorithm into Haskell yields the below code.

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = sort as ++ [x] ++ sort bs where
    (as, bs) = partition (<= x) xs

partition _ [] = ([], [])
partition p (x:xs) = let (as, bs) = partition p xs in
    if p x then (x:as, bs) else (as, x:bs)
\end{lstlisting}

The concept of parition is very cirtical to quick sort. Partition is also very important to many
other sort algorithms. We'll explain how it generally affects the sorting methodology by the end
of this chapter. Before further discussion about fine tuning of quick sort specific partition, let's
see how to realize it in-place imperatively.

There many partition methods. The one given by Nico Lomuto \cite{pearls} \cite{CLRS} will be used here as it's
easy to understand. We'll show other partition algorithm soon and see how it affects the performance.

\begin{figure}[htbp]
   \centering
   \subfloat[Partition invariant]{\includegraphics[scale=0.5]{img/partition-1-way.ps}} \\
   \subfloat[Start]{\includegraphics[scale=0.5]{img/partition-1-way-start.ps}} \\
   \subfloat[Finish]{\includegraphics[scale=0.5]{img/partition-1-way-finish.ps}}
   \caption{Partition a range of array by using the left most element as pivot.} 
   \label{fig:partition-1-way}
\end{figure}

Figure \ref{fig:partition-1-way} shows the idea of this one-pass partition method. The array is processed from
left to right. At any time, the array consists of the following parts as shown in figure \ref{fig:partition-1-way} (a): 

\begin{itemize}
\item The left most cell contains the pivot; By the end of the partition process, the pivot will be moved to the 
final proper position;
\item A segment contains all elements which are not greater than the pivot. The right boundary of this segment is
marked as `left';
\item A segment contains all elements which are greater than the pivot. The right boundary of this segment is marked
as `right'; It means that elements between `left' and `right' marks are greater than the pivot;
\item The rest of elements after `right' marks haven't been processed yet. They may greater than the
pivot or not.
\end{itemize}

At the beginning of partition, the `left' mark points to the pivot and the `right' mark points to the 
the second element next to the pivot in the array as in Figure
\ref{fig:partition-1-way} (b); Then the algorithm repeatedly advances the right mark one elements after the other
till pass the end of the array. 

In every iteration, the element pointed by the `right' mark is compared with the
pivot. If it is greater than the pivot, it should be among the segment between the `left' and `right' marks, so that
the algorithm goes on to advance the `right' mark and examine the next element; Otherwise, since the element pointed
by `right' mark is less than or equal to the pivot (not greater than), it should be put before the `left' mark.
In order to achieve this, the `left' mark needs be advanced by one, then exchange the elements pointed by the `left' 
and `right' marks.

Once the `right' mark passes the last element, it means that all the elements have been processed. The elements 
which are greater than the pivot have been moved to the right hand of `left' mark while the others are to the
left hand of this mark. Note that the pivot should move between the two segments. An extra exchanging between the pivot and
the element pointed by `left' mark makes this final one to the correct location. This is shown by the swap 
bi-directional arrow in figure \ref{fig:partition-1-way} (c).

The `left' mark (which points the pivot finally) partitions the whole array into two parts, it is 
returned as the result. We typically increase the `left' mark by one, so that it points to the
first element greater than the pivot for convinient. Note that the array is modified in-place.

The partition algorithm can be described as the following. It takes three arguments, the array $A$, the lower
and the upper bound to be partitioned \footnote{The partition algorithm used here is slightly different from
the one in \cite{CLRS}. The latter uses the last element in the slice as the pivot.}.

\begin{algorithmic}[1]
\Function{Partition}{A, l, u}
  \State $p \gets A[l]$  \Comment{the pivot}
  \State $L \gets l$ \Comment{the left mark}
  \For{$R \in [l+1, u)$} \Comment{iterate on the right mark}
    \If{$\lnot (p < A[R])$} \Comment{negate of $<$ is enough for strict weak order}
      \State $L \gets L + 1$
      \State \textproc{Exchange} $A[L] \leftrightarrow A[R]$
    \EndIf
    \State \textproc{Exchange} $A[L] \leftrightarrow p$
  \EndFor
  \State \Return $L + 1$ \Comment{The parition position}
\EndFunction
\end{algorithmic}

Below table shows the steps of paritioning the array $\{ 3, 2, 5, 4, 0, 1, 6, 7\}$.

\begin{tabular}{ | c c c c c c c c | l |}
\hline
(l) {\bf 3} & (r) 2 & 5 & 4 & 0 & 1 & 6 & 7 & initialize, $pivot = 3, l = 1, r = 2$ \\
{\bf 3} & (l) 2 & (r) 5 & 4 & 0 & 1 & 6 & 7 & $5 > 3$, moves on \\
{\bf 3} & (l) 2 & 5 & (r) 4 & 0 & 1 & 6 & 7 & $4 > 4$, moves on \\
{\bf 3} & (l) 2 & 5 & 4 & (r) 0 & 1 & 6 & 7 & $0 < 3$ \\
{\bf 3} & 2 & (l) 0 & 4 & (r) 5 & 1 & 6 & 7 & Advances $l$, then swap with $r$ \\
{\bf 3} & 2 & (l) 0 & 4 & 5 & (r) 1 & 6 & 7 & $1 < 3$ \\
{\bf 3} & 2 & 0 & (l) 1 & 5 & (r) 4 & 6 & 7 & Advances $l$, then swap with $r$ \\
{\bf 3} & 2 & 0 & (l) 1 & 5 & 4 & (r) 6 & 7 & $6 > 3$, moves on \\
{\bf 3} & 2 & 0 & (l) 1 & 5 & 4 & 6 & (r) 7 & $7 > 3$, moves on \\
1 & 2 & 0 & 3 & (l+1) 5 & 4 & 6 & 7 & $r$ passes the end, swap pivot and $l$ \\
\hline
\end{tabular}

This version of partition algoritm can be implemented in ANSI C as the following.
\lstset{language=C}
\begin{lstlisting}
int partition(Key* xs, int l, int u) {
    int pivot, r;
    for (pivot = l, r = l + 1; r < u; ++r)
        if (!(xs[pivot] < xs[r])) {
            ++l;
            swap(xs[l], xs[r]);
        }
    swap(xs[pivot], xs[l]);
    return l + 1;
}  
\end{lstlisting}

Where \verb|swap(a, b)| can either be defined as function or a macro. In ISO C++, \verb|swap(a, b)|
is provided as a function template. the type of the elements can be defined somewhere or abstracted
as a template parameter in ISO C++. We omit these language specific details here.

With the in-place partition realized, the imperative in-place quick sort can be accomplished by using it.

\begin{algorithmic}[1]
\Procedure{Quick-Sort}{$A, l, u$}
  \If{$l < u$}
    \State $m \gets$ \Call{Partition}{$A, l, u$}
    \State \Call{Quick-Sort}{$A, l, m - 1$}
    \State \Call{Quick-Sort}{$A, m, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

When sort an array, this procedure is called by passing the whole range as the lower and upper bounds.
\textproc{Quick-Sort}($A, 1, |A|+1$). Note that when $l \geq u$ it means the array slice is empty, so
the algorithm does nothing in such case.

Below ANSI C example program completes the basic in-place quick sort.

\lstset{language=C}
\begin{lstlisting}
void quicksort(Key* xs, int l, int u) {
    int m;
    if (l < u) {
        m = partition(xs, l, u);
        quicksort(xs, l, m - 1);
        quicksort(xs, m, u);
    }
}  
\end{lstlisting}

\subsection{Minor improvement in functional partition}
Before exploring how to improve the parition for basic versio quick sort, it's obviously that the 
one presented so far can be defined by using folding. Please refer to the appendix of this book for 
definition of folding.

\be
parition(p, L) = fold(f(p), (\Phi, \Phi), L)
\ee

Where function $f$ compares the element with the pivot with predicate $p$ (which is passed to $f$ as a parameter, so that
$f$ is in curried form, see appendix for detail. Alternativly, $f$ can be a lexical closure which is in
the scope of $partition$, so that it can access the predicate in this scope.), 
and update the result pair accordingly.

\be
f(p, x, (A, B)) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (\{ x \} \cup A, B) & p(x) \\
  (A, \{ x \} \cup B) & otherwise \lnot p(x)
  \end{array}
\right.  
\ee

Note we actually uses pattern-matching style definition. In environment without pattern-matching support,
the pair $(A, B)$ should be reprensted by a variable, for example $P$, and use access functions
to extract its first and second parts.

The example Haskell program needs to be modified accordingly.

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []                                
sort (x:xs) = sort small ++ [x] ++ sort big where
  (small, big) = foldr f ([], []) xs
  f a (as, bs) = if a <= x then (a:as, bs) else (as, a:bs)  
\end{lstlisting}

\subsubsection{Accumulated partition}
The partition algorithm by using folding actually accumulates to the result lists pair $(A, B)$. That
if the element is not greater than the pivot, it's accumulated to $A$, otherwise to $B$. We can explicit
express it which saves spaces and is friendly for tail-recusive call optimization (refer to the appendix
of this book for detail).

\be
partition(p, L, A, B) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (A, B) & L = \Phi \\
  partition(p, L', \{ l_1 \} \cup A, B) & p(l_1) \\
  partition(p, L', A, \{ l_1 \} \cup B) & otherwise
  \end{array}
\right.  
\ee

Where $l_1$ is the first element in $L$ if $L$ isn't empty, and $L'$ is the rest elements except for
$l_1$, that $L' = \{ l_2, l_3, ...\}$ for example.
The quick sort algorithm then uses this accumulated partition fucntion by passing the $\lambda_x x \leq pivot$
as the partition predicate.

\be
sort(L) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & L = \Phi \\
  sort(A) \cup \{ l_1 \} \cup sort(B) & otherwise
  \end{array}
\right.  
\ee

Where $A, B$ are computed by the accumulated paritition function defined above.

\[
(A, B) = partition(\lambda_x x \leq l_1, L', \Phi, \Phi)
\]

\subsubsection{Accumulated quick sort}
Observe the recursive case in the last quick sort definition. the list concatenation operations $sort(A) \cup \{l_1\} \cup sort(B)$
actually are proportion to the length of the list to be concatenated. Of course we can use some general solutions
introduced in the appendix of this book to improve it. Another way is to change the sort algorithm to accumulated
manner. Something like below:

\[
sort'(L, S) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  S & L = \Phi \\
  ... & otherwise
  \end{array}
\right.  
\]

Where $S$ is the accumulator, and we call this version by passing empty list as the accumulator to start sorting: 
$sort(L) = sort'(L, \Phi)$.
The key intuitive is that after the partition finishes, the two sub lists need to be recursively sorted. We can
first recurisvly sort the list contains the elements which are greater than the pivot, then link the pivot infront
of it and use it as a {\em accumulator} for next step sorting.

Based on this idea, the '...' part in above definition can be realized as the following.

\[
sort'(L, S) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  S & L = \Phi \\
  sort(A, \{l_1\} \cup sort(B, ?)) & otherwise
  \end{array}
\right. 
\]

The problem is what's the accumulator when sorting $B$. There is an important invariant acctually, that at
every time, the accumulator $S$ hold the elements have been sorted so far. So that we should sort $B$ by
accumulating to $S$. 

\be
sort'(L, S) =  \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  S & L = \Phi \\
  sort(A, \{l_1\} \cup sort(B, S)) & otherwise
  \end{array}
\right. 
\ee

The following Haskell example program implements the accumulated quick sort algorithm.

\lstset{language=Haskell}
\begin{lstlisting}
asort xs = asort' xs []
  
asort' [] acc = acc
asort' (x:xs) acc = asort' as (x:asort' bs acc) where
  (as, bs) = part xs [] []
  part [] as bs = (as, bs)
  part (y:ys) as bs | y <= x = part ys (y:as) bs
                    | otherwise = part ys as (y:bs)  
\end{lstlisting}

\begin{Exercise}
\begin{itemize}
\item Implement the recursive basic quick sort algorithm in your favorite imperative programming language.
\item One minor improvement is that besides the empty case, we needn't sort the singleton list (or array), implement
this idea in both imperative and functional approaches.
\item The accumulated quick sort algorithm developed in this section uses intermediate variable $A, B$. They
can be eleminated by defining the partition function mutually recurisve call the sort function. Implement this
idea in your favorite functional programming lanugage. Please don't refer to the downloadable example program
along with this book before you try it.
\end{itemize}  
\end{Exercise}

\section{Performance analysis for quick sort}

Quick sort performs well in practice, however, given theoritical analysis isn't easy. It need the tool
of probability to prove the average case performance.

Nevertheless, it's intuitive to calculate the best case and worst case peformance. It's obviously that
the best case happens when every partition divides the sequence into two slices with equal size. Thus
it takes $O(\lg N)$ recursive calls as shown in figure \ref{fig:qsort-best}.

\begin{figure}[htbp]
 \centering
 \includegraphics[scale=0.6]{img/qsort-best.ps}
 \caption{In the best case, quick sort divides the sequence into two slices with same length.}
 \label{fig:qsort-best}
\end{figure}

There are total $O(\lg N)$ levels of recursive. In the first level, it executes one partition, which
 processes $N$ elements; In the second level, it executes partition two times, each processes $N/2$
elements, so the total time in the second level bounds to $2 O(N/2) = O(N)$ as well. In the third
level, it executes partition four times, each processes $N/4$ elements. The total time in the third
level is also bound to $O(N)$; ... In the last level, there are $N$ small slices each contains a
single element, the time is bound to $O(N)$. Summing all the time in each level gives the total
performance of quick sort in best case as $O(N \lg N)$.

However, in the worst case, the partition process unluckly divides the sequence to two slices 
with unbalanced lengths in most time. That one slices with length $O(1)$, the other is $O(N)$.
Thus the resursive time degrades to $O(N)$. If we draw a similar figure, unlike in the best
case, which forms a balanced binary tree, the worst case degrades into a very unbalanced tree
that every node has only one child, while the other is empty. The binary tree turns to be
a linked list with $O(N)$ length. And in every level, all the elements are processed, so the
totoal performance in worst case is $O(N^2)$, which is as same poor as insertion sort and
selection sort.

Let's consider when the worst case will happen. One speical case is that  
all the elemnts (or most of the elements) are same. Nico Lomuto's parition method deals
with such sequence poor. We'll see how to solve this problem by introducing other
partition algorithm in the next section.

The other two obvious case which leads to worst case is that the sequence has already in
ascending or descending order. Partition the ascending sequence makes an empty sub list 
before the pivot, while the list after the pivot contains all the rest elements.
Partition the descending sequence gives an opportent result.

There are other cases which lead quick sort performs poor. There is no completely solution
which can avoid the worst case. We'll see some engineering practice in next section which can 
make it very seldom to meet the worst case.

\subsection{Average case analysis $\star$}

In average case, quick sort performs well. There is a vivid example that even the partition
divides the list every time to two lists with length 1 to 9. The performance is still bound
to $O(N \lg N)$ as shown in \cite{CLRS}.

This subsection need some mathematic background, reader can safely skip to next part.

There are two methods to proof the average case performance, one uses an important fact
that the performance is proportion to the total comparing operations during quick sort \cite{CLRS}.
Different with the selections sort that every two elements have beem compared. Quick sort
avoid many unnecessary cmoparisons. For example supose a partition operation on list
$\{ a_1, a_2, a_3, ..., a_n\}$. Select $a_1$ as the pivot, the partition builds two sub lists
$A = \{x_1, x_2, ..., x_k\}$ and $B = \{ y_1, y_2, ..., y_{n-k-1} \}$.
In the rest time of quick sort, The element in $A$ will never be compared with any elements in $B$.

Denoted the final sorted result as $\{ a_1, a_2, ..., a_n \}$, 
this indicates that if elements $a_i < a_j$, they will not be compared
any longer if and only if some elements $a_k$ where $a_i < a_k < a_j$ has ever been selected as pivot
before $a_i$ or $a_j$ being selected as the pivot.

That is to say, the only chance that $a_i$ and $a_j$ being compared is either $a_i$ is choosen
as pivot or $a_j$ is chosen as pivot before any other elements in ordered range 
$a_{i+1} < a{i+2}, ... < a_{j-1}$ are selected.

Let $P(i, )$ represent the probability that $a_i$ and $a_j$ being compared. We have:

\be
P(i, j) = \frac{2}{j - i + 1}
\ee

Since the total compare operation can be given as:

\be
C(N) = \sum_{i=1}^{N-1}\sum_{j=i+1}^{N} P(i, j)
\ee

Note the fact that if we compared $a_i$ and $a_j$, we won't compare $a_j$ and $a_i$ again in
the quick sort  algorithm, and we never compare $a_i$ on itself. That's way we set the upper
bound of $i$ to $N-1$; and lower bound of $j$ to $i+1$.

Subtitute the probability, it yields:

\be
\begin{array}{rl}
C(N) & = \displaystyle \sum_{i=1}^{N-1}\sum_{j = i+1}^{N} \frac{2}{j - i + 1} \\
     & = \displaystyle \sum_{i=1}^{N-1}\sum_{k=1}^{N-i} \frac{2}{k+1} \\
\end{array}
\ee

Using the harmonic series \cite{wiki-harmonic}

\[
H_n = 1 + \frac{1}{2} + \frac{1}{3} + .... = \ln n + \gamma + \epsilon_n
\]

\be
C(N) = \sum_{i=1}^{N-1} O(\lg N) = O(N \lg N)
\ee

The other method to prove the average performance is to use recursive fact that
when sorting list of length $N$, the partition splits the list into two
sub list with length $i$ and $N-i-1$. The partition process itself takes $cN$
time because it examine every elements with the pivot. So we have the following
equation.

\be
T(N) = T(i) + T(N-i-1) + c N 
\ee

Where $T(n)$ is the total time when perform quick sort on list of length $n$.
Since $i$ is equally likely be any of 0, 1, ..., N-1. Take math expectation to
the equation gives:

\be
\renewcommand*{\arraystretch}{1.5}
\begin{array}{rl}
T(N) & = E(T(i)) + E(T(N-i-1)) + c N \\
     & = \displaystyle \frac{1}{N} \sum_{i=0}^{N-1}T(i) + \frac{1}{N} \sum_{i=0}^{N-1}T(N-i-1) + cN \\
     & = \displaystyle \frac{1}{N} \sum_{i=0}^{N-1}T(i) + \frac{1}{N} \sum_{j=0}^{N-1}T(j) + cN \\
     & = \displaystyle \frac{2}{N} \sum_{i=0}^{N-1}T(i) + cN 
\end{array}
\ee

Mutiply by $N$ to both sides the equation changes to:

\be
N T(N) = 2 \sum_{i=0}^{N-1} T(i) + c N^2
\label{eq:ntn}
\ee

Subtitute $N$ to $N-1$ gives another eqaution:

\be
(N-1) T(N-1) = 2 \sum_{i=0}^{N-2} T(i) + c (N-1)^2
\label{eq:n1tn1}
\ee

Sustract euqation (\ref{eq:ntn}) and (\ref{eq:n1tn1}) can eleminate all the $T(i)$ for $0 \leq i < N-1$.

\be
N T(N) = (N + 1) T(N-1) + 2c N - c
\ee

As we can drop the constant time $c$ in computing performance. The equation can be extra changed like
below.

\be
\frac{T(N)}{N+1} = \frac{T(N-1)}{N} + \frac{2c}{N+1}
\ee

Next we assign $N$ to $N-1$, $N-2$, ..., which gives us $N-1$ equations.

\[
\frac{T(N-1)}{N} = \frac{T(N-2)}{N-1} + \frac{2c}{N}
\]

\[
\frac{T(N-2)}{N-1} = \frac{T(N-3)}{N-2} + \frac{2c}{N-1}
\]

...

\[
\frac{T(2)}{3} = \frac{T(1)}{2} + \frac{2c}{3}
\]

Sum all them up, and eleminate same components in both sides, we can deduce to a function of $N$.

\be
\frac{T(N)}{N+1} = \frac{T(1)}{2} + 2c \sum_{k=3}^{N+1} \frac{1}{k}
\ee

Using the harmonic series mentioned above, the final result is:

\be
O(\frac{T(N)}{N+1}) = O(\frac{T(1)}{2} + 2c \ln N + \gamma + \epsilon_N) = O(\lg N)
\ee

Thus

\be
O(T(N)) = O(N \lg N)
\ee

\begin{Exercise}
\begin{itemize}
\item Why Lomuto's methods performs poor when there are many duplicated elements?
\end{itemize}  
\end{Exercise}

% ================================================================
% Minor Improvement for quick sort
% ================================================================

\section{Engineering Improvement}

Quick sort performs well in most cases as mentioned in previous section. However, there
do exist worst case which downgrades the performance to quadratic. If the data is random
prepared, such case is rare, however, there are some particular sequences which lead to 
the worst case and these kinds of sequences are very common in practice.

In this section, some engineering practices are introduces which either help to avoid poor performance
in handling some special input data by improved partition algoirthm, or tries to uniforms
the possibilities among cases. 

\subsection{Engineering soltuion to duplicted elements}
As presented in the exercise in above section, N. Lomuto's partition method isn't good at handling
sequence with many duplicated elements. Consider a sequence with $N$ equal elements like: $\{x, x, ..., x\}$.
There are actually two methods to sort it.

\begin{enumerate}
\item The normal basic quick sort: That we select an arbitrary element, which is $x$ as the pivot, partition
the sequence to two sub list, one is $\{x, x, ..., x \}$ which contains $N-1$ elements, the other is empty.
then recursive sort the first one; this is obviously quadratic $O(N^2)$ solution.
\item The other way is that, we pick only those elements which are strictly small than $x$, and greater than $x$.
Such partition results two empty lists, and $N$ pivot. Next we recursively sort the sub lists contains
smaller elements and bigger elements, since both of them are empty, to the recursive call returns immediately;
The only thing left is to concat the recursive sort result in front of and after the pivots list.
\end{enumerate}

The latter one performs in $O(N)$ time if all elements are equal. This indicates an important improvement
for partition. That instead of binary parition (split to two sub lists and a pivot), tenery parition (split
to three sub lists) handles duplicated elements better.

We can define a ternary quick sort as the following.

\be
sort(L) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  \Phi & L = \Phi \\
  sort(S) \cup sort(E) \cup sort(G) & otherwise
  \end{array}
\right. 
\ee

Where $S, E, G$ are sub lists contains all elements which are less than, equal to, and greater than the pivot
respectively.

\[
\begin{array}{l}
S = \{ x | x \in L, x < l_1 \} \\
E = \{ x | x \in L, x = l_1 \} \\
G = \{ x | x \in L, l_1 < x \}
\end{array}
\]

The basic ternary quick sort can be implemented in Haskell as the following example code.

\lstset{language=Haskell}
\begin{lstlisting}
sort [] = []
sort (x:xs) = sort [a | a<-xs, a<x] ++ 
              x:[b | b<-xs, b==x] ++ sort [c | c<-xs, c>x]  
\end{lstlisting}

Note that here we need the comparison between elements must support abstract `less-than' and
`equal-to' operations. The basic version of ternary sort concats the three sub lists by 
concatenation, which are all linear time opertions. They can be elemintate by using the standard
technics of accumulator.

Suppose function $sort'(L, A)$ is the accumulated tenery quick sort definition, that $L$ is the sequence
to be sorted, and the accumuloator $A$ contains the intermeidate sorted result so far.
We intalize the sorting with an empty accumulator: $sort(L) = sort'(L, \Phi)$.

It's easy to give the trivial edge cases like below.

\[
sort'(L, A) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & L = \Phi \\
  ... & otherwise
  \end{array}
\right. 
\]

For the recusive case, as the ternary partition splits to three sub lists $S, E, G$, only $S$ and $G$
need recursive sort, $E$ contains all elements equal to the pivot, which is in correct order thus
needn't to be sorted any more. The idea is to sort $G$ with accumulator $A$, then concat it behind
$E$, then use this result as the new accumulator, and start to sort $S$:

\be
sort'(L, A) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  A & L = \Phi \\
  sort(S, E \cup sort(G, A)) & otherwise
  \end{array}
\right. 
\ee

The partition can also be realized with accumulators which is similar to what has been developed with
basic version of quick sort. Note that we can't just pass one predication for pivot comparison as 
we actually need two, one for less-than, the other for equality testing. For the sake of brevity,
we pass the pivot element instead.

\be
partition(p, L, S, E, G) = \left \{
  \begin{array}
  {r@{\quad:\quad}l}
  (S, E, G) & L = \Phi \\
  partition(\{l_1\} \cup S, E, G) & l_1 < p \\
  partition(S, \{l_1\} \cup E, G) & l_1 = p \\
  partition(S, E, \{ l_1 \} \cup G) & p < l_1
  \end{array}
\right. 
\ee

Below Haskell program implements this algorithm. It eleminate the intermediate variables to represent
$S, E, G$ by mutually call the sort function recursively.

\lstset{language=Haskell}
\begin{lstlisting}
sort xs = sort' xs []

sort' []     r = r
sort' (x:xs) r = part xs [] [x] [] r where
    part [] as bs cs r = sort' as (bs ++ sort' cs r)
    part (x':xs') as bs cs r | x' <  x = part xs' (x':as) bs cs r
                             | x' == x = part xs' as (x':bs) cs r
                             | x' >  x = part xs' as bs (x':cs) r
\end{lstlisting}

Richard Bird developed another version in \cite{fp-pearls}, that instead of concatenate the 
recursivly sorted results, it holds list of sorted sub list, and perform concatenation
finally.

\lstset{language=Haskell}
\begin{lstlisting}
sort xs = concat $ pass xs []

pass [] xss = xss
pass (x:xs) xss = step xs [] [x] [] xss where
    step [] as bs cs xss = pass as (bs:pass cs xss)
    step (x':xs') as bs cs xss | x' <  x = step xs' (x':as) bs cs xss
                               | x' == x = step xs' as (x':bs) cs xss
                               | x' >  x = step xs' as bs (x':cs) xss  
\end{lstlisting} %$

\subsubsection{2-way partition}

The cases with many duplicated elements can also be handled imperatively. Robert Sedgewick presented a partition
method \cite{qsort-impl}, \cite{pearls}
which holds two pointers one moves from left to right, the other moves from right to left. The two pointers
are initialized from the left and right boundary of the array.

When start parition, the left most element is selected as the pivot. Then the left pointer $i$
keeps advancing to right until it meets any element which is not less than the pivot; On the other hand\footnote{We don't use `then' because it's quite OK to perform the two scans in parallel.}, The right pointer $j$ repeatedly scan to
left until it meets any element which is not greater than the pivot.

At this time, all elemnts before the left pointer $i$ are strictly less than the pivot, while all
elements after the right pointer $j$ are greater than the pivot. $i$ points to an element which is
 either greater or equal to the pivot; while $j$ points to an element which is either less than or
equal to the pivot, the situation at this stage is illustrated in figure \ref{fig:partition-2-way} (a).

In order to partition all elements less than or equal to the pivot to the left, and the others to the right,
we can exchange the two elements pointed by $i$, and $j$. After that the scan can be resumed until either
$i$ meets $j$, or they are overlapped.

At any time point during partition. There is an invariant that all elements before $i$ (including the one 
pointed by $i$) are not greater than
the pivot; while all elements after $j$ (including the one pointed by $j$) are not less than the pivot. 
The elements between $i$ and $j$ haven't been examined yet. This invariant is shown in figure \ref{fig:partition-2-way} (b).

\begin{figure}[htbp]
   \centering
   \subfloat[When pointer $i$, and $j$ stop]{\includegraphics[scale=0.5]{img/partition-2-way-inner.ps}} \\
   \subfloat[Partition invariant]{\includegraphics[scale=0.5]{img/partition-2-way.ps}} \\
   \caption{Partition a range of array by using the left most element as pivot.} 
   \label{fig:partition-2-way}
\end{figure}

After the left pointer $i$ meets the right pointer $j$, or they overlaps eachother, we need one extra exchanging
to move the pivot located at the first position to the correct place which is pointed by $j$. Next, the 
elements between the lower bound and $j$ as well as the sub slice between $i$ and the upper bound of the array
are recursively sorted.

This algorithm can be described as the following.

\begin{algorithmic}
\Procedure{Sort}{$A, l, u$} \Comment{sort range $[l, u)$}
  \If{$u - l > 1$} \Comment{More than 1 element for non-trivial case}
    \State $i \gets l$, $j \gets u$
    \State $pivot \gets A[l]$
    \Loop
      \Repeat
        \State $i \gets i + 1$
      \Until{$A[i] \geq pivot$} \Comment{Need handle error case that $i \geq u$ in fact.}
      \Repeat
        \State $j \gets j - 1$
      \Until{$A[j] \leq pivot$} \Comment{Need handle error case that $j < l$ in fact.}
      \If{$j < i$}
        \State break
      \EndIf
      \State \textproc{Exchange} $A[i] \leftrightarrow A[j]$
    \EndLoop
    \State \textproc{Exchange} $A[l] \leftrightarrow A[j]$ \Comment{Move the pivot}
    \State \Call{Sort}{$A, l, j$}
    \State \Call{Sort}{$A, i, u$}
  \EndIf
\EndProcedure
\end{algorithmic}

Consider the extreme case that all elements are equal, this in-place quick sort will partition
the list to two equal length sub lists although it takes $\frac{N}{2}$ unecessary swappings.
As the partition is balanced, the overall performance is $O(N \lg N)$, which avoid downgrading
to quadratic. The following ANSI C example program implements this algorithm.

\lstset{language=C}
\begin{lstlisting}
void qsort(Key* xs, int l, int u) {
    int i, j, pivot;
    if (l < u - 1) {
        pivot = i = l; j = u;
        while (1) {
            while (i < u && xs[++i] < xs[pivot]);
            while (j >=l && xs[pivot] < xs[--j]);
            if (j < i) break;
            swap(xs[i], xs[j]);
        }
        swap(xs[pivot], xs[j]);
        qsort(xs, l, j);
        qsort(xs, i, u);
    }
}
\end{lstlisting}

\section{Engineering solution to worst case}
Random quick sort , 3 median, ...

\section{Other engineering practice}
TODO: Revert to insertion sort: Sedgewick's 

\section{Side words}
TODO: Relationship between quick sort and tree sort (deforestration, Richard Bird)

% ================================================================
% Merge Sort
% ================================================================

\section{Merge sort}


\section{Short summary} 
From easy/hard partition, easy/hard merge point of view

TODO:

\begin{thebibliography}{99}

\bibitem{TAOCP}
Donald E. Knuth. ``The Art of Computer Programming, Volume 3: Sorting and Searching (2nd Edition)''. Addison-Wesley Professional; 2 edition (May 4, 1998) ISBN-10: 0201896850 ISBN-13: 978-0201896855

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein. 
``Introduction to Algorithms, Second Edition''. ISBN:0262032937. The MIT Press. 2001

\bibitem{qsort-impl}
Robert Sedgewick. ``Implementing quick sort programs''. Communication of ACM. Volume 21, Number 10. 1978. pp.847 - 857.

\bibitem{pearls}
Jon Bentley. ``Programming pearls, Second Edtion''. Addison-Wesley Professional; 1999. ISBN-13: 978-0201657883

\bibitem{fp-pearls}
Richard Bird. ``Pearls of functional algorithm design''. Cambridge University Press. 2010. ISBN, 1139490605, 9781139490603

\bibitem{wiki-qs}
Wikipedia. ``Quicksort''. http://en.wikipedia.org/wiki/Quicksort

\bibitem{wiki-sweak-order}
Wikipedia. ``Strict weak order''. http://en.wikipedia.org/wiki/Strict\_weak\_order

\bibitem{wiki-total-order}
Wikipedia. ``Total order''. http://en.wokipedia.org/wiki/Total\_order

\bibitem{wiki-harmonic}
Wikipedia. ``Harmonic series (mathematics)''. http://en.wikipedia.org/wiki/Harmonic\_series\_(mathematics)

\end{thebibliography}

\ifx\wholebook\relax\else
\end{document}
\fi
