\ifx\wholebook\relax \else
% ------------------------ 

\documentclass{article}
%------------------- Other types of document example ------------------------
%
%\documentclass[twocolumn]{IEEEtran-new}
%\documentclass[12pt,twoside,draft]{IEEEtran}
%\documentstyle[9pt,twocolumn,technote,twoside]{IEEEtran}
%
%-----------------------------------------------------------------------------
%\input{../../../common.tex}
\input{../../../common-en.tex}

\setcounter{page}{1}

\begin{document}

\fi
%--------------------------

% ================================================================
%                 COVER PAGE
% ================================================================

\title{Binary Heaps with Functional and imperative implementation}

\author{Liu~Xinyu
\thanks{{\bfseries Liu Xinyu } \newline
  Email: liuxinyu95@gmail.com \newline}
%  Tel:   +86-1305-196-8666 \newline}
  }

\markboth{Binary Heaps}{Imperative and Functional}

\maketitle

\ifx\wholebook\relax
\chapter{Binary Heaps with Functional and imperative implementation}

\section{abstract}
\else
\begin{abstract}
\fi
Heap is one of the elementary data structure. It is widely used
to solve some practical problems, such as sorting, prioritized
scheduling, and graph algorithms\cite{wiki-heap}. 

Most popular implementations of heap are using a kind of implicit
binary heap by array, which is described in ``Introduction to 
Algorithm'' textbook\cite{CLRS}. Examples include C++/STL
heap and Python heapq.

However, heaps can be generalized and implemented with varies
of other data structure besides array. In this post, explicit
binary tree is used to realize heaps. It leads to Leftist heaps, Skew heaps,
and Splay heaps, which are suitable for pure functional implementation as shown
by Okasaki\cite{okasaki-book}.

There are multiple programming languages used, including
C++, Haskell, Python and Scheme/Lisp.

There may be mistakes in the post, please feel free to point out.

This post is generated by \LaTeXe, and provided with GNU FDL(GNU Free Documentation License).
Please refer to http://www.gnu.org/copyleft/fdl.html for detail.

\ifx\wholebook\relax \else
\end{abstract}
\fi

\vspace{3cm}
{\bfseries Keywords:} Binary Heaps, Leftist Heaps, Skew Heaps, Splay Heaps

%{\bfseries Corresponding Author:} Liu Xinyu

\maketitle

% ================================================================
%                 Introduction
% ================================================================
\section{Introduction}
\label{introduction}

Heap is an important elementary data structure. Most of the algorithm
textbooks introduce heap, especially about binary heap and heap sort.

Some popular implementation, such as C++/STL heap and Python heapq
are based on binary heaps (implicit binary heap by array
more precisely). And the fastest heap sort algorithm is also
written with binary heap as poposed by R. W. Floyd
\cite{wiki-heapsort} \cite{rosetta-heapsort}.

In this post, we use a general definition of heap, so that
varies of under-ground data structures can be used for implementation.
And binary heap is also extended to a wide concept under this definition.

A heap is a data structure that satisifies the following {\em heap property}.
\begin{itemize}
\item Top operation always returns the minimum (maximum) element;
\item Pop operation removes the top element from the heap while the heap
property should be kept, so that the new top element is still the 
minimum (maximum) one;
\item Insert a new element to heap should keep the heap property. That
the new top is still the minimum (maximum) element;
\item Other operations including merge etc should all keep the heap property.
\end{itemize}

This is a kind of recursive definition, while it doesn't limit the under
ground data structure.

We call the heap with top returns the minimum element {\em min-heap},
while if top returns the maximum element, we call it {\em max-heap}.

In this post, I'll first give the definition of binary heap. Then
I'll review the traditional imperative way of implicit heap by array.
After that, by considering explicit heap by binary trees, I'll
explain the Leftist heap, Skew heap, Splay heap, and provide pure functional implementation
for them based on Okasaki's result\cite{okasaki-book}.

As the last part, the computation complexity will be mentioned, and
I'll show in what situation, Leftist heap performs bad.

I'll introduce some other heaps, including Binomial heaps, Fibonacci
heaps, and Paring heaps in a seperate post.

This article provides example implementation in C++, Haskell, Python, and 
Scheme/Lisp languages. 

All source code can be downloaded in appendix \ref{appendix}, please 
refer to appendix for detailed information about build and run.


% ================================================================
%                 Implicit binary heap by array
% ================================================================
\section{Implicit binary heap by array}
\label{ibheap}

Considering the heap definition in previous section, one option to 
implement heap is by using trees. A straightforward solution is
to store the minimum (maximum) element in the root node of the 
tree, so for `top' operation, we simply return the root as the
result. And for `pop' operation, we can remove the root and 
rebuild the tree from the children.

If the tree which is used to implement heap is a binary tree, we
can call it {\em binary heap}. There are three types of binary
heap implementation explained in this post. All of them are
based on binary tree.

% ================================================================
%                 Definition
% ================================================================
\subsection{Definition}

The first one is implicit binary tree indeed. Consider the problem
how to represent a complete binary tree with array. (For example, try to 
represent a complete binary tree in a programming language doesn't support structure
or record data type, so that only array can be used). One solution
is to pack all element from top level (root) down to bottom level (leaves).

Figure \ref{fig:tree-array-map} shows a complete binary tree and
its corresponding array representation.

\begin{figure}[htbp]
       \begin{center}
       	  \includegraphics[scale=0.5]{img/tree-array-map-tree.ps}
          \includegraphics[scale=0.5]{img/tree-array-map-array.ps}
        \caption{Mapping between a complete binary tree and array} \label{fig:tree-array-map}
       \end{center}
\end{figure}

This mapping relationship between tree and array can be denote as 
the following equations (Note that the array index starts from 1).

\begin{algorithmic}[1]
\Function{PARENT}{$i$}
  \State \Return $\lfloor \frac{i}{2} \rfloor$
\EndFunction
\Statex
\Function{LEFT}{$i$}
  \State \Return $2i$
\EndFunction
\Statex
\Function{LEFT}{$i$}
  \State \Return $2i+1$
\EndFunction
\end{algorithmic}

For a given tree node which is represented as the $i$-th element in an
array, since the tree is complete, we can easily find its parent node
as the $\lfloor i/2 \rfloor$-th element in the array; Its left child
with index of $2i$ and right child with index of $2i+1$. If the index
of the child exceeds the length of the array, it simply means this 
node don't have such child.

This mapping calculation can be performed fast if bit-wise operation
is used.

\subsection*{Definition of implicit binary heap by array in C++}
In C++ language, array index starts from zero, but not one. The mapping
from array to binary tree should be adjusted accordingly.

\lstset{language=C++}
\begin{lstlisting}
template<class T>
T parent(T i){ return ((i+1)>>1)-1; }

template<class T>
T left(T i){ return (i<<1)+1; }

template<class T>
T right(T i){ return (i+1)<<1; }
\end{lstlisting}

The tepe T must support bit-wise operation.

\subsection*{Definition of implicit binary heap by array in Python}
Similar as C/C++, the array index in Python starts from 0, so we provide
the mapping functions as below.

\lstset{language=Python}
\begin{lstlisting}
def parent(i):
    return (i+1)//2-1

def left(i):
    return  2*i+1

def right(i):
    return 2*(i+1)
\end{lstlisting}

% ================================================================
%                 Heapify
% ================================================================
\subsection{Heapify}

The most important thing for heap algorithm is to maintain the heap
property, that the top element should be the minimum (maximum) one.

For the implicit binary heap by array, it means for a given node,
which is represented as the $i$-th index, we must develop a algorithm
to check if all its two children conform to this property and incase
there is violation, we need swap the parent and child to fix the 
problem. 

In ``Introduction to Algorithms'' book\cite{CLRS}, this algorithm
is given in a recursive way, here we show a pure imperative solution.
Let's take min-heap for example.

\begin{algorithmic}[1]
\Function{HEAPIFY}{$A, i$}
  \State $n \gets LENGTH(A)$
  \Loop
    \State $l \gets LEFT(i)$
    \State $r \gets RIGHT(i)$
    \State $smallest \gets i$
    \If{$l < n$ and $A[l] < A[i]$}
      \State $smallest \gets l$
    \EndIf
    \If{$r < n$ and $A[r] < A[smallest]$}
      \State $smallest \gets r$
    \EndIf
    \If{$smallest \neq i$}
      \State $exchange A[i] \leftrightarrow A[smallest]$
      \State $i \gets smallest$
    \Else
      \State \Return
    \EndIf
  \EndLoop
\EndFunction
\end{algorithmic}

This algorithm assume that for a given node, the children all conform 
to the heap property, however, we are not sure if the value of this node 
is the smallest compare to its tow children.

For array $A$ and a given index $i$, we need check none its left child or right child
is bigger than $A[i]$, in case we find violation, we pick the smallest one, and set
it as the new value for $A[i]$, the previous value of $A[i]$ is then set as the
new value of the child, and we need go along the child tree to repeat this check and fixing
process until we either reach a leaf node or there is no heap property violation.

Note that the $HEAPIFY$ algorithm takes $O(\lg{N})$ time.

\subsection*{Heapify in C++}

For C++ program, we need it cover both traditional C compatible array, and
the modern container abstraction. There are several options to realize this
requirement.

One method is to pass iterators as argument to heap algorithm. C++/STL
implementation (at the time the author wrote this post) uses this approach.

The advantage of using iterator is that some random access iterator is
just implemented as C pointers, so it compatible with C array well.

However, this method need us change the algorithm from a array index style
to pointer operation style. It's hard to reflect the above pseudo code
quite clear in such style. Because of this problem, we won't use this approach
here. Reader can refer to STL source code for detailed information.

Another option is to pass the array or container as well as the 
number of elements as arguments. However, we need abstract the 
comparison anyway so that the algorithm works for both max-heap
and min-heap.

\lstset{language=C++}
\begin{lstlisting}
template<class T> struct MinHeap: public std::less<T>{};
template<class T> struct MaxHeap: public std::greater<T>{};
\end{lstlisting}

Here we define MinHeap and MaxHeap as a kind of alias of less-than
and greater-than logic comparison functor template.

The heapify algorithm can be implemented as the following.

\begin{lstlisting}
template<class Array, class LessOp>
void heapify(Array& a, unsigned int i, unsigned int n, LessOp lt){
  while(true){
    unsigned int l=left(i);
    unsigned int r=right(i);
    unsigned int smallest=i;
    if(l < n && lt(a[l], a[i]))
      smallest = l;
    if(r < n && lt(a[r], a[smallest]))
      smallest = r;
    if(smallest != i){
      std::swap(a[i], a[smallest]);
      i = smallest;
    }
    else
      break;
  }
}
\end{lstlisting}

The program accepts the reference of the array (both reference
of the container, and reference to pointer are OK), the index
from where we want to adjust so that all children of it confirms
to the heap property; the number of elements in the array, and
a comparison functor. It checks from the node which is indexed
as $i$ down to the leaf until it find a node that both children are
``less than'' the value of the node based on the comparison functor. 
Otherwise, it will locate the ``smallest'' one and swap it with
the node value.

It is also possible to create a concept of range and realize the 
algorithm with it. Some C++ library, such as boost has already
support range. Here we can develop a light weight range only for
random access container.

\begin{lstlisting}
template<class RIter> // random access iterator
struct Range{
  typedef typename std::iterator_traits<RIter>::value_type value_type;
  typedef typename std::iterator_traits<RIter>::difference_type size_t;
  typedef typename std::iterator_traits<RIter>::reference  reference;
  typedef RIter iterator;

  Range(RIter left, RIter right):first(left), last(right){}

  reference  operator[](size_t i){ return *(first+i); }
  size_t size() const { return last-first; }
  
  RIter first;
  RIter last;
};
\end{lstlisting}

For a given left index $l$, and right index $r$, a range represents
$[l, r)$, So it is easy to construct a range with iterators as well
as the pointer of the array and its length.

Two overloaded auxiliary function templates are provided to create
range easily.

\begin{lstlisting}
template<class Iter>
Range<Iter> range(Iter left, Iter right){ return Range<Iter>(left, right); }

template<class Iter>
Range<Iter> range(Iter left, typename Range<Iter>::size_t n){
  return Range<Iter>(left, left+n);
}
\end{lstlisting}

The above algorithm can be implemented with range like below.

\begin{lstlisting}
template<class R, class LessOp>
void heapify(R a, typename R::size_t i, LessOp lt){
  typename R::size_t l, r, smallest;
  while(true){
    l = left(i);
    r = right(i);
    smallest = i;
    if( l < a.size() && lt(a[l], a[i]))
      smallest = l;
    if( r < a.size() && lt(a[r], a[smallest]))
      smallest = r;
    if( smallest != i){
      std::swap(a[i], a[smallest]);
      i = smallest;
    }
    else
      break;
  }
}
\end{lstlisting}

Almost everything is as same as the former one except that the number
of elements can be given by the size of the range.

In order to verify the program, a same test case as in CLRS figure 6.2
is fed to our function.

\begin{lstlisting}
// test c-array
const int a[] = {16, 4, 10, 14, 7, 9, 3, 2, 8, 1};
const unsigned int n = sizeof(a)/sizeof(a[0]);
int x[n];
std::copy(a, a+n, x);
heapify(x, 1, n, MaxHeap<int>());
print_range(x, x+n);

// test random access container
std::vector<short> y(a, a+n);
heapify(y, 1, n, MaxHeap<short>());
print_range(y.begin(), y.end());
\end{lstlisting}

The same test case can also be applied to the ``range'' version of
program.

\begin{lstlisting}
heapify(range(x, n), 1, MaxHeap<int>());

//...

heapify(range(y.begin(), y.end()), 1, MaxHeap<short>());
\end{lstlisting}

Where ``print\_range'' is a helper function to output all elements
in a container, a C array or a range.

\begin{lstlisting}
template<class Iter>
void print_range(Iter first, Iter last){
  for(; first!=last; ++first)
    std::cout<<*first<<", ";
  std::cout<<"\n";
}

template<class R>
void print_range(R a){
  print_range(a.first, a.last);
}
\end{lstlisting}

The above test code can output a result as below:

\begin{verbatim}
16, 14, 10, 8, 7, 9, 3, 2, 4, 1, 
16, 14, 10, 8, 7, 9, 3, 2, 4, 1,
\end{verbatim}

Figure \ref{fig:heapify} shows how this algorithm works.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=0.3]{img/heapify-1.ps}

    a. Step 1, 14 is the biggest element among 4, 14, and 7. Swap 4 with the left child;

    \includegraphics[scale=0.3]{img/heapify-2.ps}

    b. Step 2, 8 is the biggest element among 2, 4, and 8. Swap 4 with the right child;

    \includegraphics[scale=0.3]{img/heapify-3.ps}

    c. 4 is the leaf node. It hasn't any children. Process terminates.
    \caption{Heapify example, a max-heap case.} \label{fig:heapify}
  \end{center}
\end{figure}

\subsection*{Heapify in Python}

In order to cover both min-heap and max-heap, we abstract the comparation
as the following lambda expressions.

\lstset{language = Python}
\begin{lstlisting}
MIN_HEAP = lambda a, b: a < b
MAX_HEAP = lambda a, b: a > b 
\end{lstlisting}

By passing the above defined comparation operation as a parameter, the ``Heapify''
algorithm is given as below.

\begin{lstlisting}
def heapify(x, i, less_p = MIN_HEAP):
    n = len(x)
    while True:
        l = left(i)
        r = right(i)
        smallest = i
        if l < n and less_p(x[l], x[i]):
            smallest = l
        if r < n and less_p(x[r], x[smallest]):
            smallest = r
        if smallest != i:
            (x[i], x[smallest])=(x[smallest], x[i])
            i  = smallest
        else:
            break
\end{lstlisting}

We can use the same test case as presents in Figure 6.2 of \cite{CLRS}.

\begin{lstlisting}
l = [16, 4, 10, 14, 7, 9, 3, 2, 8, 1]
heapify(l, 1, MAX_HEAP)
print l 
\end{lstlisting}

The result is something like this.

\begin{verbatim}
[16, 14, 10, 8, 7, 9, 3, 2, 4, 1] 
\end{verbatim}

This result is as same as the one presented in \ref{fig:heapify}.

% ================================================================
%                 Build a heap
% ================================================================
\subsection{Build a heap}

With heapify algorithm, it is easy to build a heap from an arbitary 
array. Observe that the number of nodes in a complete binary tree
for each level is a list like:

$1, 2, 4, 8, ..., 2^i, ...$.

The only exception is the last level. Since the tree may not full
(note that complete binary tree doesn't mean full binary tree), the
last level contains at most $2^{p-1}$ nodes, where $2^p \leq n$ and $n$
is the length of the array.

Heapify algorithm doesn't take any effect on leave node, which means
we can skip applying heapify for all nodes. In other words,  
all leaf nodes have already satisfied heap property. We only need 
start checking and maintaining heap property from the last branch node.
the Index of the last branch node is no greater than $\lfloor n/2 rfloor$.

Based on this fact, we can build a heap with the following algorihm.
(Assume the heap is min-heap).

\begin{algorithmic}[1]
\Function{BUILD-HEAP}{$A$}
  \State $n \gets LENGTH(A)$
  \For{$i \gets \lfloor n/2 \rfloor$ downto $1$}
    \State $HEAPIFY(A, i)$
  \EndFor
\EndFunction
\end{algorithmic}

Altough the complexity of $HEAPIFY$ is $O(\lg{N})$, the running time
of $BUILD\_HEAP$ doesn't bound to $O(N \lg{N})$ but to $O(N)$, so this
is a linear time algorithm. Please refer to \cite{CLRS} for the 
detailed proof.

\subsection*{Build a heap in C++}

The only adjustment in C++ program from the above algorithm is
about the starting index from 1 to 0. 

\lstset{language=C++}
\begin{lstlisting}
template<class Array, class LessOp>
void build_heap(Array& a, unsigned int n, LessOp lt){
  unsigned int i = (n-1)>>1;
  while(true){
    heapify(a, i, n, lt);
    if(i==0) break; // this is a trick: unsigned int always >=0
    --i;
  }
}
\end{lstlisting}

Note that since the unsigned type is used to represent index,
It can't lower than zero. We can't just use a for loop as below.

\begin{lstlisting}
for(unsigned int i = (n-1)>>1; i>=0; --i) //wrong, i always >=0
\end{lstlisting}

This program can be easily adjusted with range concept.

\begin{lstlisting}
template<class RangeType, class LessOp>
void build_heap(RangeType a, LessOp lt){
  typename RangeType::size_t i = (a.size()-1)>>1;
  while(true){
    heapify(a, i, lt);
    if(i==0) break;
    --i;
  }
}
\end{lstlisting}

We can test our program with the same data as in Figure 6.3 in \cite{CLRS}.

\begin{lstlisting}
// test c-array
const int a[] = {4, 1, 3, 2, 16, 9, 10, 14, 8, 7};
const unsigned int n = sizeof(a)/sizeof(a[0]);
int x[n];
std::copy(a, a+n, x);
build_heap(range(x, n), MaxHeap<int>());
print_range(x, x+n);

// test random access container
std::vector<int> y(a, a+n);
build_heap(range(y.begin(), y.end()), MaxHeap<short>());
print_range(y.begin(), y.end());
\end{lstlisting}

Running results are printed in console like the following.

\begin{verbatim}
16, 14, 10, 8, 7, 9, 3, 2, 4, 1, 
16, 14, 10, 8, 7, 9, 3, 2, 4, 1,
\end{verbatim}

Figure \ref{fig:build-heap-1} and \ref{fig:build-heap-2} 
show the steps when build a heap from
an arbitary array. The node in black color is the one we will apply
$HEAPIFY$ algorithm, the nodes in gray color are swapped during
$HEAPIFY$.

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=0.3]{img/build-heap-array.ps}

    a. An array in arbitary order before build-heap process;

    \includegraphics[scale=0.3]{img/build-heap-1.ps}

    b. Step 1, The array is mapped to binary tree. The first branch node, which is
16 is to be examined;

    \includegraphics[scale=0.3]{img/build-heap-2.ps}

    c. Step 2, 16 is the largest element in current sub tree, next is to check node
with value 2;
    
    \caption{Build a heap from an arbitary array. Gray nodes are changed in each step,
black node is the one to be processed next step.} \label{fig:build-heap-1}
  \end{center}
\end{figure}

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=0.3]{img/build-heap-3.ps}

    d. Step 3, 14 is the largest value in the subtree, swap 14 and 2; next is to check
node with value 3;

    \includegraphics[scale=0.3]{img/build-heap-4.ps}

    e. Step 4, 10 is the largest value in the subtree, swap 10 and 3; next is to check
node with value 1;

    \includegraphics[scale=0.3]{img/build-heap-5.ps}

    f. Step 5, 16 is the largest value in current node, swap 16 and 1 first; then
similarly, swap 1 and 7; next is to check the root node with value 4;

    \includegraphics[scale=0.3]{img/build-heap-6.ps}

    g. Step 6, Swap 4 and 16, then swap 4 and 14, and then swap 4 and 8; 
And the whole build process finish.

    \caption{Build a heap from an arbitary array. Gray nodes are changed in each step,
black node is the one to be processed next step.} \label{fig:build-heap-2}
  \end{center}
\end{figure}


\subsection*{Build a heap in Python}

Like the C++ heap building program, we check from the last non-leaf
node and apply $HEAPIFY$ algorithm, and repeat the process back to
the root node. However, we use explicit calculation (divided by 2)
instead of using bit-wise shifting.

\lstset{language=Python}
\begin{lstlisting}
def build_heap(x, less_p = MIN_HEAP):
    n = len(x)
    for i in reversed(range(n//2)):
        heapify(x, i, less_p)
\end{lstlisting}

We can feed the similar test case to this program as below:

\begin{lstlisting}
l = [4, 1, 3, 2, 16, 9, 10, 14, 8, 7]
build_heap(l, MAX_HEAP)
print l
\end{lstlisting}

It will output the same result as the C++ program.

\begin{verbatim}
[16, 14, 10, 8, 7, 9, 3, 2, 4, 1]
\end{verbatim}

% ================================================================
%                 Basic heap operations
% ================================================================
\subsection{Basic heap operations}

From the generic definition of heap (not neccessarily binary heap),
It's essential to provides basic operations so that user can access
the data and modify it.

The most important operations included accessing the top element 
(find the minimum or maximum element), pop one element (the minimum
one or the maximum one depends on the type of the heap)
from the heap, find the top N elements, decrease a key (note this 
is for min-heap, and it will be increase a key for max-heap), and
insertion.

For binary tree, most of this operation is bound to $O(\lg{N})$ worst-case,
some of them, such as top is $O(1)$ time.

\subsubsection{Access the top element (minimum)}
According to the definition of heap, there must be an operation to
return the top element. for implicit binary tree by array, it is the
root node which stores the minimum (maximum) value.

\begin{algorithmic}[1]
\Function{TOP}{$A$}
  \State \Return $A[0]$
\EndFunction
\end{algorithmic}

This operation is trivial. It takes $O(1)$ time.

\subsubsection*{Access the top element in C++}

By translate the above algorithm directly in C++, we get the 
following program.

\lstset{language=C++}
\begin{lstlisting}
template<class T>
typename ValueType<T>::Result heap_top(T a){ return a[0]; }
\end{lstlisting}

There is a small trick to get the type of the element store in 
array no matter if the array is STL container or plain C like
array.

\begin{lstlisting}
template<class T> struct ValueType{
  typedef typename T::value_type Result;
};

template<class T> struct ValueType<T*>{
  typedef T Result; // c-pointer type
};

template<class T, unsigned int n> struct ValueType<T[n]>{
  typedef T Result; // c-array type
};
\end{lstlisting}

Not that the C++ template meta programming support to
specialize for a certain type.

Here we skip the error handling of empty heap case. If the 
heap is empty, one option is just to raise exception.

\subsubsection*{Access the top element in Python}

The python version of this progam is also simple, we omit
the error handling for empty heap as well.

\lstset{language=C++}
\begin{lstlisting}
def heap_top(x):
    return x[0] #ignore empty case
\end{lstlisting}

\subsubsection{Heap Pop (delete minimum)}

Different from the top operation, pop operation is a bit
complex, because the heap property has to be maintained
after the top element is removed.

The solution is to apply $HEAPIFY$ algorithm immediately to the
next node to the root node which has been removed.

A quick but slow algorithm based on this idea may look like
the following.

\begin{algorithmic}[1]
\Function{POP-SLOW}{$A$}
  \State $x \gets TOP(A)$
  \State $REMOVE(A, 1)$
  \If{$A$ is not empty}
    \State $HEAPIFY(A, 1)$
  \EndIf
  \State \Return $x$
\EndFunction
\end{algorithmic}

This algorihm first remember the top element in $x$, then
it removes the first element from the array, the size of
this array reduced by one. After that if the array isn't 
empty, $HEAPIFY$ will applied to the modified array on
the first element (previous the second element).

Removing an element from array takes $O(N)$ time,
where $N$ is the length of the array. Removing the first
element need shift all the rest values one by one.
Because of this bottle neck, it slows the whole algorihtm
to $O(N)$.

In order to solve this program, one alternative way is 
to just swap the first element and the last one in the
array, then shrink the array size by one.

\begin{algorithmic}[1]
\Function{POP}{$A$}
  \State $x \gets TOP(A)$
  \State $SWAP(A[1], A[HEAP-SIZE(A)])$
  \State $REMOVE(A, HEAP-SIZE(A))$
  \If{$A$ is not empty}
    \State $HEAPIFY(A, 1)$
  \EndIf
  \State \Return $x$
\EndFunction
\end{algorithmic}

Note that remove the last element from the array takes 
only $O(1)$ time, and $HEAPIFY$ is bound to $O(\lg{N})$.
The whole algorithm is bound to $O(\lg{N})$ time.

\subsubsection*{Pop in C++}

In C++ program, we abstract the min-heap and max-heap as heap type
template parameter, and pass it explicitly.

First is the `reference + size' approach.

\lstset{language=C++}
\begin{lstlisting}
template<class T, class LessOp>
typename ValueType<T>::Result heap_pop(T& a, unsigned int& n, LessOp lt){
  typename ValueType<T>::Result top = heap_top(a);
  a[0] = a[n-1];
  heapify(a, 0, --n, lt);
  return top;
}
\end{lstlisting}

And it can be adapted to ``range'' abstraction as well.

\begin{lstlisting}
template<class R, class LessOp>
typename R::value_type heap_pop(R& a, LessOp lt){
  typename R::value_type top = heap_top(a);
  std::swap(a[0], a[a.size()-1]);
  --a.last;
  heapify_(a, 0, lt);
  return top;
}
\end{lstlisting}

\subsubsection*{Pop in Python}

Python provides pop() function to get rid of the last element,
so the program can be developed as below.

\begin{lstlisting}
def heap_pop(x, less_p = MIN_HEAP):
    top = heap_top(x)
    x[0] = x[-1] # this is faster than top = x.pop(0)
    x.pop()
    if x!=[]:
        heapify(x, 0, less_p)
    return top
\end{lstlisting}

\subsubsection{Find the first $K$ biggest (smallest) element}

With pop operation, it is easy to implement algorithm to
find the top $K$ elements. In order to find the biggest $K$
values form an array, we can build a max-heap, then perform
pop operation $K$ times.

\begin{algorithmic}[1]
\Function{TOP-K}{$A, k$}
  \State $BUILD-HEAP(A)$
  \For{$i \gets 1, MIN(k, LENGTH(A))$}
    \State $APPEND(Result, POP(A))$
  \EndFor
  \State \Return $Result$
\EndFunction
\end{algorithmic}

Note that if $K$ is bigger than the length of the array, it means
we need return the whole array as the result. That's why it need
use the $MIN$ function in the algorithm.

\subsubsection*{Find the first $K$ biggest (smallest) element in C++}

In C++ program, we can pass the iterator for output, so the 
``reference - size'' version looks like below.

\lstset{language=C++}
\begin{lstlisting}
template<class Iter, class Array, class LessOp>
void heap_top_k(Iter res, unsigned int k, 
                Array& a, unsigned int& n, LessOp lt){
  build_heap(a, n, lt);
  unsigned int count = std::min(k, n);
  for(unsigned int i=0; i<count; ++i)
    *res++=heap_pop(a, n, lt);
}
\end{lstlisting}

When we adapt to `range' concept, it is possible to manipulate
the data in place, so that we can put the top $K$ element
in first $K$ positions in the array.

\begin{lstlisting}
template<class R, class LessOp>
void heap_top_k(R a, typename R::size_t k, LessOp lt){
  typename R::size_t count = std::min(k, a.size());
  build_heap(a, lt);
  while(count--){
    ++a.first;
    heapify(a, 0, lt);
  }
}
\end{lstlisting}

The algorithm doesn't utilize `pop' function, instead, after
the heap is built, the first elemetn is the top one, it 
adjusts the range one position next, then apply heapify
to the new range. This process is repeated for $K$ times
so the first $K$ elements are the result.

A simple test cases can be fed to the program for verification.

\begin{lstlisting}
const int a[] = {4, 1, 3, 2, 16, 9, 10, 14, 8, 7};
unsigned int n = sizeof(a)/sizeof(a[0]);
std::vector<int> x(a, a+n);
heap_top_k(range(x.begin(), x.end()), 3, MaxHeap<int>());
print_range(range(x.begin(), 3));
\end{lstlisting}

The result is printed in console like below.

\begin{verbatim}
16, 14, 10,
\end{verbatim}

\subsubsection*{Find the first $K$ biggest (smallest) element in Python}

In Python we can put `pop' function to list comprehension to
get the top $K$ elements like the following.

\lstset{language=Python}
\begin{lstlisting}
def top_k(x, k, less_p = MIN_HEAP):
    build_heap(x, less_p)
    return [heap_pop(x, less_p) for i in range(min(k, len(x)))]
\end{lstlisting}

The testing and result are shown as the following.

\begin{lstlisting}
l = [4, 1, 3, 2, 16, 9, 10, 14, 8, 7] 
res = top_k(l, 3, MAX_HEAP)
print res
\end{lstlisting}

Evaluate the code led to below line.

\begin{lstlisting}
[16, 14, 10]
\end{lstlisting}

\subsubsection{Modification: Decrease key}

Heap can be used to implement priority queue, because of this, it
is important to modify the key stored in heap. One typical operation
is to increase the priority of a tasks so that it can be performed
earlier.

Here we present the decrease key operation for a min-heap. The
corresponding operation is increase key for max-heap.

Once we modified a key by decreasing it in a min-heap, it can make
the node conflict with the heap property, that the key may be less
than some values in its ancestors. In order to maintain the
invariant, an auxiliary algorithm is provided to fix the heap
property.

\begin{algorithmic}[1]
\Function{HEAP-FIX}{$A, i$}
  \While{$i>1$ and $A[i] < A[PARENT[i]]$}
    \State Exchange $A[i] \leftrightarrow A[PARENT[i]]$
    \State $i \gets PARENT[i]$
  \EndWhile
\EndFunction
\end{algorithmic}

This algorithm repeatly examine the key of parent node and
the key in current node. It will swap nodes in case the
parent contains the smaller key. This process is performed
from current node towards the root node till it find that
the parent node holds the smaller key.

With this auxiliary algorithm, decrease key can be realized
easily.

\begin{algorithmic}[1]
\Function{DECREASE-KEY}{$A, i, k$}
  \If{$k < A[i]$}
    \State $A[i] \gets k$
    \State $HEAP-FIX(A, i)$
  \EndIf
\EndFunction
\end{algorithmic}

Note that the algorithm only takes effect when the new key
is less than the original key.

\subsubsection*{Decrease key in C++}
In order to support both min-heap and max-heap, the comparision
function object is passed as a parameter in C++ implementation.

\lstset{language=C++}
\begin{lstlisting}
template<class Array, class LessOp>
void heap_fix(Array& a, unsigned int i, LessOp lt){
  while(i>0 && lt(a[i], a[parent(i)])){
    std::swap(a[i], a[parent(i)]);
    i = parent(i);
  }
}

template<class Array, class LessOp>
void heap_decrease_key(Array& a, 
                       unsigned int i, 
                       typename ValueType<Array>::Result key,
                       LessOp lt){
  if(lt(key, a[i])){
    a[i] = key;
    heap_fix(a, i, lt);
  }
}
\end{lstlisting}

Some very simiple verification case can be fed to the program.
Here we use the example presented in \cite{CLRS} Figure 6.5.

\begin{lstlisting}
const int a[] = {16, 14, 10, 8, 7, 9, 3, 2, 4, 1};
const unsigned int n = sizeof(a)/sizeof(a[0]);
int x[n];
std::copy(a, a+n, x);
heap_decrease_key(x, 8, 15, MaxHeap<int>());
print_range(x, x+n);
\end{lstlisting}

Run the above lines will genreate the following output.
\begin{verbatim}
16, 15, 10, 14, 7, 9, 3, 2, 8, 1,
\end{verbatim}

In this max-heap example, we try to increase the key of the 9th node from 
4 to 15. As shown in figure \ref{fig:decrease-key}

\begin{figure}[htbp]
  \begin{center}
    \includegraphics[scale=0.3]{img/decrease-key-a.ps}

    a. The 9-th node with key 4 will be modified;

    \includegraphics[scale=0.3]{img/decrease-key-b.ps}

    b. The key is modified to 15, which is greater than its parent;

    \includegraphics[scale=0.3]{img/decrease-key-c.ps}

    c. According the max-heap property, 8 and 15 are swapped.

    \includegraphics[scale=0.3]{img/decrease-key-d.ps}

    d. Since 15 is greater than 14, which is the key of its parent node, 15 and 14 are swapped. Because 15 is less than 16, the algorithm terminates.
    
    \caption{Example process when increase a key in a max-heap.} \label{fig:decrease-key}
  \end{center}
\end{figure}

\subsubsection*{Decrease key in Python}
The Python version decrease key program is similar as well. It first
checks if the new key is ``less than'' the original one, if yes it
modifies the value, and peforms the fixing process.

\lstset{language=Python}
\begin{lstlisting}
def heap_decrease_key(x, i, key, less_p = MIN_HEAP):
    if less_p(key, x[i]):
        x[i] = key
        heap_fix(x, i, less_p)

def heap_fix(x, i, less_p = MIN_HEAP):
    while i>0 and less_p(x[i],x[parent(i)]):
        (x[parent(i)], x[i]) = (x[i], x[parent(i)])
        i = parent(i)
\end{lstlisting}

We can use the same test case to verify the program.

\begin{lstlisting}
l = [16, 14, 10, 8, 7, 9, 3, 2, 4, 1]
heap_decrease_key(l, 8, 15, MAX_HEAP)
print l
\end{lstlisting}

It will output the same result as the C++ program.

\begin{verbatim}
[16, 15, 10, 14, 7, 9, 3, 2, 8, 1]
\end{verbatim}

\subsubsection{Insertion}

In \cite{CLRS}, insertion is implemented by using $DECREASE-KEY$.
The approach is to first insert a node with infinity key. According
to the min-heap property, the node should be the last element
in the under ground array. After that, the key is decreased to
the value to be inserted, so that we can call decrease-key to
finish the process.

Instead of reuse $DECREASE-KEY$, we can reuse $HEAP-FIX$ to implement
insertion. The new key is directly appended at the end of the array,
and the $HEAP-FIX$ is applied on this new node.

\begin{algorithmic}[1]
\Function{HEAP-PUSH}{$A, k$}
  \State $APPEND(A, k)$
  \State $HEAP-FIX(A, SIZE(A))$
\EndFunction
\end{algorithmic}

\subsubsection*{Insertion by decreasing key method in C++}
In C++, traditional C array is static sized, so appending a new element
to the array has to be managed properly. In order to simplify the
problem, we assume the neccessary has already been allocated (by
client program).

First is the ``reference + size'' version of program.

\lstset{language=C++}
\begin{lstlisting}
template<class Array, class LessOp>
void heap_push(Array& a, 
               unsigned int& n,
               typename ValueType<Array>::Result key,
               LessOp lt){
  a[n] = key; 
  heap_fix(a, n, lt);
  ++n;
}
\end{lstlisting}

Note that the size is explicitly increased. so after calling this
function, the count of the element is increased by one.

It is also possible to prvoide equivalent program by using `range'.

\begin{lstlisting}
template<class R, class LessOp>
void heap_push(R a, typename R::value_type key, LessOp lt){
  *a.last++ = key;
  heap_fix(a, a.size()-1, lt);
}
\end{lstlisting}

We can test the insert program with a very simple case.

\begin{lstlisting}
const int a[] = {16, 14, 10, 8, 7, 9, 3, 2, 4, 1};
unsigned int n = sizeof(a)/sizeof(a[0]);
std::vector<int> x(a, a+n);
x.push_back(0);
heap_push(range(x.begin(), n), 17, MaxHeap<int>());
print_range(x.begin(), x.end());
\end{lstlisting}

Note that, in cient program (this test program), we
reserved the memory in advance, or it will cause
access violation problem. 
Running these lines will output the following result.

\begin{verbatim}
17, 16, 10, 8, 14, 9, 3, 2, 4, 1, 7,
\end{verbatim}

We can found the new element 17 is inserted at the proper
position of the heap.

\subsubsection*{Insertion directly in Python}

In python program, append a new element to a list is build-in
supported. So client program doesn't need to take care of the
similar problem as described in C++ implementation.

\lstset{language=Python}
\begin{lstlisting}
def heap_insert(x, key, less_p = MIN_HEAP):
    i = len(x)
    x.append(key)
    heap_fix(x, i, less_p)
\end{lstlisting}

If the same test case is fed to the above function, we can 
get the output like the following.

\begin{lstlisting}
l = [16, 14, 10, 8, 7, 9, 3, 2, 4, 1]
heap_insert(l, 17, MAX_HEAP)
print l
\end{lstlisting}

\begin{verbatim}
[17, 16, 10, 8, 14, 9, 3, 2, 4, 1, 7
\end{verbatim}

% ================================================================
%                 Heap sort
% ================================================================
\subsection{Heap sort}
\label{heap-sort}

Heap sort algorithm is an interesting application of heap. According
to the heap property, the min(max) element can be easily accessed
by from the top of the heap. So a straightforward way to sort an
arbitary of values is to first build a heap from them, then continously
pops the smallest element from the heap till the heap is empty.

The algorithm based on this strategy is something like below.

\begin{algorithmic}[1]
\Function{HEAP-SORT}{$A$}
  \State $R \gets NIL$
  \State $BUILD-HEAP(A)$
  \While{$A \neq NIL$}
    \State $APPEND(R, HEAP-POP(A))$
  \EndWhile
  \State \Return $R$
\EndFunction
\end{algorithmic}

Robert. W. Floyd found a very fast implementation of heap sort.
The idea is to build a max-heap instead of min-heap, so the first
element is the biggest one. Then this biggest element is swapped
with the last element in the array, so that it is in the right
position after sorting. Now the last element becomes the top
of the heap, it may violate the heap property. We can perform
$HEAPIFY$ on it with the heap size shrink by one. This process
is repeated till there is only one element left in the heap.

\begin{algorithmic}[1]
\Function{HEAP-SORT-FAST}{$A$}
  \State $BUILD-MAX-HEAP(A)$
  \While{$SIZE(A) > 1$}
    \State Exchange $A[1] \leftrightarrow A[SIZE(A)]$
    \State $SIZE(A) \gets SIZE(A) - 1$
    \State $HEAPIFY(A, 1)$
  \EndWhile
\EndFunction
\end{algorithmic}

Note that this algorithm is in-place algorithm. It's the
fastest heap sort algorithm by far.

In terms of complexity, $BUILD-HEAP$ is bound to $O(N)$. 
Since $HEAPIFY$ is $O(\lg{N})$, and it
is called $O(N)$ times, so both above algorithms take $O(N \lg{N})$
timt to run.

\subsection*{Floyd's heap sort algorithm in C++}

Only Floyd's algorithm is given in C++ in this post. 

\lstset{language=C++}
\begin{lstlisting}
template<class Array, class GreaterOp>
void heap_sort(Array& a, unsigned int n, GreaterOp gt){
  for(build_heap(a, n, gt); n>1; --n){
    std::swap(a[0], a[n-1]);
    heapify(a, 0, n-1, gt);
  }
}
\end{lstlisting}

A very simple test case is used for verification.

\begin{lstlisting}
const int a[] = {16, 14, 10, 8, 7, 9, 3, 2, 4, 1};
std::vector<int> y(a, a+n);
heap_sort(range(y.begin(), y.end()), MaxHeap<int>());
print_range(y.begin(), y.end());
\end{lstlisting}

The result is output as we expect.

\begin{verbatim}
1, 2, 3, 4, 7, 8, 9, 10, 14, 16,
\end{verbatim}

\subsection*{General heap sort algorithm in Python}

Instead of Floyd method, we'll show the straightforward
`popping $N$ times' algorithm in Python. Please refer
to \cite{rosetta-heapsort} for Floyd algorithm in Python.

\lstset{language=Python}
\begin{lstlisting}
def heap_sort(x, less_p = MIN_HEAP):
    res = []
    build_heap(x, less_p)
    while x!=[]:
        res.append(heap_pop(x, less_p))
    return res
\end{lstlisting}

And the test case is as same as the one we used in C++ program.

\begin{lstlisting}
l = [16, 14, 10, 8, 7, 9, 3, 2, 4, 1]
res = heap_sort(l)
print res
\end{lstlisting}

The result is output as the following.

\begin{verbatim}
[1, 2, 3, 4, 7, 8, 9, 10, 14, 16]
\end{verbatim}

% ================================================================
%                 Explicit binary heap
% ================================================================
\section{Leftist heap and Skew heap, explicit binary heaps}
\label{ebheap}

Instead of using implicit binary tree by array, it is natural to 
consider why we can't use explicit binary tree to realize heap?

There are some problems must be solved if we turn into explicit
binary tree as the under ground data strucutre for heap.

The first problem is about the $HEAP-POP$ or $DELETE-MIN$ operation.
If the explicit binary tree is represent as the form of 
(left value right), which is shown in figure \ref{fig:lvr}

\begin{figure}[htbp]
       \begin{center}
       	  \includegraphics[scale=1]{img/lvr.ps}
        \caption{A binary tree, all values in left child and right child are smaller than $k$.} \label{fig:lvr}
       \end{center}
\end{figure}

If $k$ is the top element, all values in left and right children are less
than $k$. After $k$ is popped, only left and right children are left.
They have to be merged to a new tree. Since heap property should be maintained
after merge, so the new root element is the smallest one.

Since both left child and right child are heaps in binary tree, the two trivial
cases can be found immediately.

\begin{algorithmic}[1]
\Function{MERGE}{$L, R$}
  \If{$L = NIL$}
    \State \Return $R$
  \ElsIf{$R = NIL$}
    \State \Return $L$
  \Else
    \State $...$
  \EndIf
\EndFunction
\end{algorithmic}

If neither left child nor right child is empty tree, because they all fit
heap property, the top element of them are all minimum value respectively.
One solution is to compare the root value of the left and right children,
select the smaller one as the new root of the merged heap, and recursively
merget the other child to one of the children of the smaller one. 
For instance if $L = (A x B)$ and $R = (A' y B')$, where $A, A', B, B'$ 
are all sub trees, and $x < y$. There are two candidate results according
to this strategy.

\begin{itemize}
\item $(MERGE(A, R) x B)$
\item $(A x MERGE(B, R))$
\end{itemize}

Both are correct result. One simplified solution is only merge on right
sub tree. Leftist tree provides a systematically approach based on this
idea.

% ================================================================
%                 Definition
% ================================================================
\subsection{Definition}

The heap implemented by Leftist tree is called Leftist heap. Leftist
tree is first introduced by C. A. Crane in 1972\cite{wiki-leftist-tree}.

\subsubsection{Rank (S-value)}

In Leftist tree, a rank value (or $S$ value) is defined to each node. 
Rank is the distance to the nearest external node. Where external node
is a NIL concept extended from leaf node.

For example, in figure \ref{fig:rank}, the rank of NIL 
is defined 0, consider the root node 4, The nearest leaf node is 
the child of node 8. So the rank of root node 4 is 2. Becasue node 
6 and node 8 are all only contain NIL, so the rank values are 1. 
Altough node 5 has non-NIL left child, However, since the right 
child is NIL, so the rank value, which is the minimum distance 
to leaf is still 1.

\begin{figure}[htbp]
   \begin{center}
     \includegraphics[scale=0.5]{img/rank.ps}
     \caption{rank(4) = 2, rank(6) = rank(8) = rank(5) = 1.} \label{fig:rank}
   \end{center}
\end{figure}

\subsubsection{Leftist property}

With rank defined, we can create a strategy when merging.

\begin{itemize}
\item Every time when merging, we always merge to right child; Denote the rank
of the new right sub tree as $r_r$;
\item Compare the ranks of the left and right children, if the rank of 
left sub tree is $r_l$ and $r_l < r_r$, we swap the left and right children.
\end{itemize}

We call this `Leftist property'. Gernerally speaking, a Lefitst tree always
has the shortest path to an external node on the right.

A Leftist tree tends to be very unbalanced, However, it ensures an important
property as specified in the following therom.

If a Leftist tree $T$ contains $N$ internal nodes, the path from root to the
rightmost external node contains at most $\lfloor \log{(N+1)} \rfloor$ nodes.

For the proof of these therom, please refer to \cite{brono-book} and \cite{TAOCP}.

With this therom, algorithms operate along this path are all bound to $O(\lg N)$.

\subsection*{Definition of Leftist heap in Haskell}

In Haskell the definition of Leftist tree is almost as same as the
binary search tree except there is a rank field added.

\lstset{language=Haskell}
\begin{lstlisting}
data LHeap a = E -- Empty 
             | Node Int a (LHeap a) (LHeap a) -- rank, element, left, right
               deriving (Eq, Show)
\end{lstlisting}

In order to access the rank feild, a helper function is provided.

\begin{lstlisting}
rank::LHeap a -> Int
rank E = 0
rank (Node r _ _ _) = r
\end{lstlisting}

\subsection*{Definition of Leftist heap in Scheme/Lisp}

TODO: ...

\lstset{language=lisp}
\begin{lstlisting}
--...
\end{lstlisting}

% ================================================================
%                 Merge
% ================================================================
\subsection{Merge}

In order to realize `merge', an auxiliary algorithm is given to 
compare the ranks and do swapping if neccessary.

\begin{algorithmic}[1]
\Function{LEFTIFY}{$T$}
  \State $l \gets LEFT(T), r \gets RIGHT(T)$
  \State $k \gets KEY(T)$
  \If{$RANK(l) < RANK(r)$}
    \State $RANK(T) \gets RANK(L) + 1$
  \Else
    \State $RANK(T) \gets RANK(R) + 1$
    \State Exchange $l \leftrightarrow r$
  \EndIf
\EndFunction
\end{algorithmic}

The algorithm compares the rank of the left and
right sub trees, pick the less one and add it by one as the 
rank of the modified node. If the rank of left side is greater,
it will also swap the left and right children.

The reason why rank need to be increased by one is because there
is a new key added on top of the tree, which causes the rank 
increase.

With $LEFTIFY$ defined, merge algorithm can be provided as the
following.

\begin{algorithmic}[1]
\Function{MERGE}{$L, R$}
  \If{$L = NIL$}
    \State \Return $R$
  \ElsIf{$R = NIL$}
    \State \Return $L$
  \Else
    \State $T \gets CREATE-NEW-NODE()$
    \If{$KEY(L) < KEY(R)$}
      \State $KEY(T) \gets KEY(L)$
      \State $LEFT(T) \gets LEFT(L)$
      \State $RIGHT(T) \gets MERGE(RIGHT(L), R)$
    \Else
      \State $KEY(T) \gets KEY(R)$
      \State $LEFT(T) \gets LEFT(R)$
      \State $RIGHT(T) \gets MERGE(L, RIGHT(R))$
    \EndIf
    \State $LEFTIFY(T)$
  \EndIf
  \State \Return $T$
\EndFunction
\end{algorithmic}

Note that $MERGE$ algorithm always operate on right side, and call
$LEFITFY$ to ensure the `Leftist property, so that this algorithm
is bound to $O(\lg N)$.

\subsection*{Merge in Haskell}

Translate the algorithm to Haskell lead to the following program.
Here we modified the pesudo code to a pure funcitonal style.

\lstset{language=Haskell}
\begin{lstlisting}
merge::(Ord a)=>LHeap a -> LHeap a -> LHeap a
merge E h = h
merge h E = h
merge h1@(Node _ x l r) h2@(Node _ y l' r') = 
    if x < y then makeNode x l (merge r h2)
    else makeNode y l' (merge h1 r')

makeNode::a -> LHeap a -> LHeap a -> LHeap a
makeNode x a b = if rank a < rank b then Node (rank a + 1) x b a
                 else Node (rank b + 1) x a b
\end{lstlisting}

\subsection*{Merge in Scheme/Lisp}

\subsubsection{Merge operation in implicit binary heap by array}

TODO: $O(N)$ algorithm

\subsubsection*{Merge operation in implicit binary heap in C++}

\subsubsection*{Merge operation in implicit binary heap in Python}

% ================================================================
%                 Basic heap operations
% ================================================================
\subsection{Basic heap operations}

Most of the basic heap operations can be implemented easily with $MERGE$
algorithm define above.

\subsubsection{Find minimum (top) and delete minimum (pop)}
Since we keep the smallest element in root node, finding the minimum
value (top element) is trivial. It's a $O(1)$ operation.

\begin{algorithmic}[1]
\Function{TOP}{$T$}
  \State \Return $KEY(T)$
\EndFunction
\end{algorithmic}

While if the top element popped, left and right children are merged
so the heap updated.

\begin{algorithmic}[1]
\Function{POP}{$T$}
  \State \Return $MERGE(LEFT(T), RIGHT(T))$
\EndFunction
\end{algorithmic}

Note that pop operation on Leftist heap takes $O(\lg N)$ time.

\subsubsection*{Find minimum (top) and delete minimum in Haskell}

We skip the error handling of operation on an empty 
Leftist heap.

\lstset{language=Haskell}
\begin{lstlisting}
findMin :: LHeap a -> a
findMin (Node _ x _ _) = x
\end{lstlisting}

\begin{lstlisting}
deleteMin :: (Ord a) => LHeap a -> LHeap a
deleteMin (Node _ _ l r) = merge l r
\end{lstlisting}

\subsubsection*{Find minimum (top) and delete minimum in Scheme/Lisp}

\subsubsection{Insertion}

To insert a new key to the heap, one solution is to create a signle
leaf node from the key, and perfom merge with this leaf node and
the Leftist tree.

\begin{algorithmic}[1]
\Function{INSERT}{$T, k$}
  \State $x \gets CREATE-NEW-NODE()$
  \State $KEY(x) \gets k$
  \State $RAKN(x) \gets 1$
  \State $LEFT(x), RIGHT(x) \gets NIL$
  \State \Return $MERGE(x, T)$
\EndFunction
\end{algorithmic}

Since insert still call merge insdie, the algorithm is bound to $O(\lg N)$
time.

\subsubsection*{Insertion in Haskell}

Translating the above algorithm to Haskell is trivial.

\lstset{language=Haskell}
\begin{lstlisting}
insert::(Ord a)=> LHeap a -> a -> LHeap a
insert h x = merge (Node 1 x E E) h
\end{lstlisting}

In order to provide a convenient way to build a Leftist heap from
a list, an auxiliary function is given as the following.

\begin{lstlisting}
fromList :: (Ord a) => [a] -> LHeap a
fromList = foldl insert E
\end{lstlisting}

This function can be used like this.

\begin{lstlisting}
fromList [9, 4, 16, 7, 10, 2, 14, 3, 8, 1]
\end{lstlisting}

It will create a Leftist heap as below.

\begin{verbatim}
Node 1 1 (Node 3 2 (Node 2 4 (Node 2 7 (Node 1 16 E E) 
(Node 1 10 E E)) (Node 1 9 E E)) (Node 2 3 (Node 1 14 E E) 
(Node 1 8 E E))) E
\end{verbatim}

Figure \ref{fig:leftist-tree} shows the result respectively.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/leftist-tree.ps}
    \caption{A Lefitst tree.} \label{fig:leftist-tree}
   \end{center}
\end{figure}

\subsubsection*{Insertion in Scheme/Lisp}



% ================================================================
%                 Heap sort
% ================================================================
\subsection{Heap sort by Leftist Heap}

With all the basic operations defined, it's straightforward to 
implement heap sort in a $N$-popping way. Once we need sort a
list of elements, we first build a Leftist heap from the list.
Then repeatly pop the minimum element from the heap until heap
is empty.

First is the algoritm to build the Leftist heap by insert all
elements to an empty heap.

\begin{algorithmic}[1]
\Function{BUILD-HEAP}{$A$}
  \State $H \gets NIL$
  \For{each $x$ in $A$}
    \State $T \gets INSERT(T, x)$
  \EndFor
  \State \Return $T$
\EndFunction
\end{algorithmic}

And the heap sort algorithm is as same as the generic one presented
in \ref{heap-sort}.

Note this algorithm is bound to $O(N \lg N)$ time.

\subsection*{Heap sort in Haskell}

In Haskell program, since we have already defined the `fromList'
auxiliar function to build Leftist heap from a list, the heap sort
algoritm can utilize it.

\lstset{language=Haskell}
\begin{lstlisting}
heapSort :: (Ord a) => [a] -> [a]
heapSort = hsort . fromList where
    hsort E = []
    hsort h = (findMin h):(hsort $ deleteMin h)
\end{lstlisting} %$

Here is an example case which is used in previous C++ and Python
programs.

\begin{lstlisting}
heapSort [16, 14, 10, 8, 7, 9, 3, 2, 4, 1]
\end{lstlisting}

It will output the same result as the following.

\begin{verbatim}
[1,2,3,4,7,8,9,10,14,16]
\end{verbatim}

\subsection*{Heap sort in Scheme/Lisp}

TODO:

% ================================================================
%                 Skew Heap
% ================================================================


\subsection{Skew heaps}
\label{skew-heap}

The problem with Letist heap is that, it performs bad in some cases.
For example, if we examine the Leftist heap behind the above heap
sort test case, it's a very unbalanced binary tree as shown in figure
\ref{fig:unbalanced-leftist-tree}\footnote{run Haskell Leftist tree
function: fromList [16, 14, 10, 8, 7, 9, 3, 2, 4, 1] will generates
the result: Node 1 1 (Node 2 2 (Node 1 3 (Node 2 7 (Node 1 8 (Node 1 10 (Node 1 14 (Node 1 16 E E) E) E) E) (Node 1 9 E E)) E) (Node 1 4 E E)) E}.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.3]{img/unbalanced-leftist-tree.ps}
    \caption{A very unbalanced Leftitst tree build from list [16, 14, 10, 8, 7, 9, 3, 2, 4, 1].} \label{fig:unbalanced-leftist-tree}
   \end{center}
\end{figure}

The binary tree is almost turned to be a linked-list. The worst case
is when feed a orderred list for buidling Leftist tree, since the 
tree changed to linked-list, the time bound degrade from $O(\lg N)$
to $O(N)$.

Skew heap (or {\em self-adjusting heap}) is one step ahead simplified Leftist heap \cite{wiki-skew-heap} \cite{self-adjusting-heaps}.

Remind the Leftist heap, we swap the left and right children during merge
when the rank on left side is less than right side. This comparision strategy 
doens't work when one of the sub tree has only one child. Because
in such case, the rank of the sub tree is always 1 no matter how
big it is. A Brute-force approach is to swap the left and right children
every time when merge. This idea leads to Skew heap.

\subsubsection{Definition of Skew heap}

A Skew heap is a heap impelemented with Skew tree. A Skew tree is a special 
binary tree. The minimum element is stored in root node. Every sub tree is 
also a skew tree.

Based on above discussion, there is no use to keep the rank (or $S$-value)
field, so the Skew heap difintion is as same as the binary tree from the
programming language point of view.

\subsubsection*{Definition of Skew heap in Haskell}

After removing rank from Leftist heap definition, we can get the Skew
heap one.

\lstset{language=Haskell}
\begin{lstlisting}
data SHeap a = E -- Empty 
             | Node a (SHeap a) (SHeap a) -- element, left, right
               deriving (Eq, Show)
\end{lstlisting}

\subsubsection*{Definition of Skew heap in Scheme/Lisp}

TODO:

\subsubsection{Merge}

The merge algorithm tends to be very simple. When we merge two Skew
trees, we compare the root element of each tree, and pick the smaller
one as the new root, we then merge the other tree contains bigger
element onto the right sub tree and swap the left and right children.

\begin{algorithmic}[1]
\Function{MERGE}{$L, R$}
  \If{$L = NIL$}
    \State \Return $R$
  \ElsIf{$R = NIL$}
    \State \Return $L$
  \Else
    \State $T \gets CREATE-EMPTY-NODE()$
    \If{$KEY(L) < KEY(R)$}
      \State $KEY(T) \gets KEY(L)$
      \State $LEFT(T) \gets MERGE(R, RIGHT(L))$
      \State $RIGHT(T) \gets LEFT(L)$
    \Else
      \State $KEY(T) \gets KEY(R)$
      \State $LEFT(T) \gets MERGE(L, RIGHT(R))$
      \State $RIGHT(T) \gets LEFT(R)$
    \EndIf
    \State \Return $T$
  \EndIf
\EndFunction
\end{algorithmic}

\subsubsection*{Skew heap in Haskell}

Translating the above algorithm into Haskell gets a simple merge program.

\lstset{language=Haskell}
\begin{lstlisting}
merge::(Ord a)=>SHeap a -> SHeap a -> SHeap a
merge E h = h
merge h E = h
merge h1@(Node x l r) h2@(Node y l' r') = 
    if x < y then Node x (merge r h2) l
    else Node y (merge h1 r') l'
\end{lstlisting}

All the rest programs are as same as the Leftist heap except we needn't
provide rank value when construct a node. 

\begin{lstlisting}
insert::(Ord a)=> SHeap a -> a -> SHeap a
insert h x = merge (Node x E E) h

findMin :: SHeap a -> a
findMin (Node x _ _) = x

deleteMin :: (Ord a) => SHeap a -> SHeap a
deleteMin (Node _ l r) = merge l r
\end{lstlisting}

If we feed a completely ordered list to Skew heap, it will results a
fairly balanced binary trees as shown in figure \ref{fig:skew-tree}.

\begin{lstlisting}
*SkewHeap>fromList [1..10]
Node 1 (Node 2 (Node 6 (Node 10 E E) E) (Node 4 (Node 8 E E) E)) 
(Node 3 (Node 5 (Node 9 E E) E) (Node 7 E E))
\end{lstlisting}

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/skew-tree.ps}
    \caption{Skew tree is still balanced even the input is an ordered list.} \label{fig:skew-tree}
   \end{center}
\end{figure}

\subsubsection*{Skew heap in Scheme/Lisp}

TODO:

% ================================================================
%                 Splay Heap
% ================================================================

\section{Splay heap, another explicit binary heap}
\label{splayheap}

Lefist heap presents that it's quite possible to implement
heap data structure with explicit binary tree. Skew heap
shows one method to solve the balance problem. Splay heap
on the other hand, shows another balance approach.

Although Leftist heap and Skew heap use binary trees, they
are not Binary Search tree (BST). If we turn the underground
data structure to binary search tree, the minimum(maximum)
element isn't located in root node. It takes $O(\lg N)$ time
to find the minimum(maximum) element.

Binary search tree becomes inefficient if it isn't well
balanced, operations degrades to $O(N)$ in the worst case.
Although it's quite OK to use red-black tree to implement
binary heap, Splay tree provides a light weight implementation
with acceptable dynamic balancing result.

% ================================================================
%                 Definition
% ================================================================
\subsection{Definition}

Splay tree uses a cache-like approach that it keeps rotating the current
access node close to the top, so that the node can be accessed fast
next time. It defines such kinds of operaiton as ``Splay''. For an 
unbalanced binary search tree, after several times of splay operation, the
tree tends to be more and more balanced. Most basic operation of 
Splay tree have amortized $O(\lg N)$ time. Splay tree was invented
by Daniel Dominic Sleator and Robert Endre Tarjan in 1985\cite{wiki-splay-tree} 
\cite{self-adjusting-trees}.

\subsubsection{Splaying}

There are two kinds of splaying approach method. The first one is
a bit complex, However, it can be implemented fairly simiple with
pattern matching. The second one is simple, but the implementation
is a bit complex.

Denote the node is currently accessed as $X$, it's parent node as $P$,
and it's grand parent node (if has) as $G$. There are 3 steps for 
splaying. Each step contains 2 symmetric cases. For illustration 
purpose, only one case is shown for each step.

\begin{itemize}
\item {\em Zig-zig step.} As shown in figure \ref{fig:zig-zig}, in this case,
both $X$, and its parent $P$ are either left children or right children. By
rotating 2 times, X becomes the new root.
 
\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/zig-zig-a.ps} 
          \includegraphics[scale=0.5]{img/zig-zig-b.ps}
          \caption{Zig-zig case.} \label{fig:zig-zig}
   \end{center}
\end{figure}

\item {\em Zig-zag step.} As shown in figure \ref{fig:zig-zag}, in this
case, $X$ is the right child of its parent while $P$ is the left child.
Or $X$ is the left child of $P$, and $P$ is the right child of $G$.
After rotation, $X$ becomes the new root, $P$ and $G$ become siblings.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/zig-zag-a.ps}
          \includegraphics[scale=0.5]{img/zig-zag-b.ps}
          \caption{Zig-zag case.} \label{fig:zig-zag}
   \end{center}
\end{figure}

\item {\em Zig step.} As shown in figure \ref{fig:zig}, in this case,
$P$ is the root, we perform one rotate, so that $X$ becomes new root.
Note this is the last step in splay operation.

\begin{figure}[htbp]
   \begin{center}
   	  \includegraphics[scale=0.5]{img/zig-a.ps}
          \includegraphics[scale=0.5]{img/zig-b.ps}
          \caption{Zig case.} \label{fig:zig}
   \end{center}
\end{figure}

\end{itemize}

Okasaki found a simple rule for Splaying, that every time we follow
two left branches in a row, or two right branches in a row, we rotate
those two nodes.

\subsection*{Definition of Splay heap in Haskell}

TODO: ...

Okasaki's method and pattern matching method

\lstset{language=Haskell}
\begin{lstlisting}
--...
\end{lstlisting}

\subsection*{Definition of Splay heap in Scheme/Lisp}

TODO: ...

\lstset{language=lisp}
\begin{lstlisting}
--...
\end{lstlisting}

% ================================================================
%                 Basic heap operations
% ================================================================
\subsection{Basic heap operations}

\subsubsection{Insertion}

\subsubsection*{Insertion in Haskell}

\subsubsection*{Insertion in Scheme/Lisp}

\subsubsection{Find minimum (top) and delete minimum (pop)}

\subsubsection*{Find minimum (top) and delete minimum in Haskell}

\subsubsection*{Find minimum (top) and delete minimum in Scheme/Lisp}

% ================================================================
%                 Heap sort
% ================================================================
\subsection{Heap sort}

TODO: code reuse

% ================================================================
%                 Short summary
% ================================================================
\section{Notes and short summary}

TODO: ...

Review to big-O, and balance problem.
Intro of N-way heaps.


% ================================================================
%                 Appendix
% ================================================================
\section{Appendix} \label{appendix}
%\appendix
All programs provided along with this article are free for
downloading.

\subsection{Prerequisite software}
GNU Make is used for easy build some of the program. For C++ and ANSI C programs,
GNU GCC and G++ 3.4.4 are used. 
For Haskell programs GHC 6.10.4 is used
for building. For Python programs, Python 2.5 is used for testing, for
Scheme/Lisp program, MIT Scheme 14.9 is used.

all source files are put in one folder. Invoke 'make' or 'make all'
will build C++ and Haskell program. 

Run 'make Haskell' will separate build Haskell program. the executable
file is ``htest'' (with .exe
in Window like OS). It is also possible to run the program in GHCi.

\subsection{Tools}

Besides them, I use graphviz to draw most of the figures in this post. In order to
translate the B-tree output to dot script. A Haskell tool is provided.
It can be used like this.

\begin{verbatim}
bt2dot filename.dot "string"
\end{verbatim}

Where filename.dot is the output file for the dot script. It can
parse the string which describes B-tree content and translate it 
into dot script.

This source code of this tool is BTr2dot.hs, it can also be downloaded 
with this article.

download position: http://sites.google.com/site/algoxy/btree/btree.zip

\begin{thebibliography}{99}

\bibitem{CLRS}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest and Clifford Stein. ``Introduction to Algorithms, Second Edition''. The MIT Press, 2001. ISBN: 0262032937.

\bibitem{wiki-heap}
Heap (data structure), Wikipedia. http://en.wikipedia.org/wiki/Heap\_(data\_structure)

\bibitem{wiki-heapsort}
Heapsort, Wikipedia. http://en.wikipedia.org/wiki/Heapsort

\bibitem{okasaki-book}
Chris Okasaki. ``Purely Functional Data Structures''. Cambridge university press, (July 1, 1999), ISBN-13: 978-0521663502

\bibitem{rosetta-heapsort}
Sorting algorithms/Heapsort. Rosetta Code. http://rosettacode.org/wiki/Sorting\_algorithms/Heapsort

\bibitem{wiki-leftist-tree}
Leftist Tree, Wikipedia. http://en.wikipedia.org/wiki/Leftist\_tree

\bibitem{brono-book}
Bruno R. Preiss. Data Structures and Algorithms with Object-Oriented Design Patterns in Java. http://www.brpreiss.com/books/opus5/index.html

\bibitem{TAOCP}
Donald E. Knuth. ``The Art of Computer Programming. Volume 3: Sorting and Searching.''. Addison-Wesley Professional; 
2nd Edition (October 15, 1998). ISBN-13: 978-0201485417. Section 5.2.3 and 6.2.3

\bibitem{wiki-skew-heap}
Skew heap, Wikipedia. http://en.wikipedia.org/wiki/Skew\_heap

\bibitem{self-adjusting-heaps}
Sleator, Daniel Dominic; Jarjan, Robert Endre. ``Self-adjusting heaps'' SIAM Journal on Computing 15(1):52-69. doi:10.1137/0215004 ISSN 00975397 (1986)

\bibitem{wiki-splay-tree}
Splay tree, Wikipedia. http://en.wikipedia.org/wiki/Splay\_tree

\bibitem{self-adjusting-trees}
Sleator, Daniel D.; Tarjan, Robert E. (1985), ``Self-Adjusting Binary Search Trees'', Journal of the ACM 32(3):652 - 686, doi: 10.1145/3828.3835

\end{thebibliography}

\ifx\wholebook\relax \else
\end{document}
\fi
